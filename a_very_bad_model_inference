{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84969,"databundleVersionId":10033515,"sourceType":"competition"},{"sourceId":9908654,"sourceType":"datasetVersion","datasetId":6087921},{"sourceId":9909120,"sourceType":"datasetVersion","datasetId":6088299},{"sourceId":10621715,"sourceType":"datasetVersion","datasetId":6576623},{"sourceId":9909200,"sourceType":"datasetVersion","datasetId":6088368},{"sourceId":245795,"sourceType":"modelInstanceVersion","modelInstanceId":210025,"modelId":231720}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install copick git+https://github.com/copick/copick-utils.git git+https://github.com/copick/DeepFindET.git\n# !pip install copick git+https://github.com/copick/copick-utils.git git+https://github.com/copick/DeepFindET.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:00:19.524805Z","iopub.execute_input":"2025-01-30T21:00:19.525474Z","iopub.status.idle":"2025-01-30T21:00:19.529340Z","shell.execute_reply.started":"2025-01-30T21:00:19.525443Z","shell.execute_reply":"2025-01-30T21:00:19.528326Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/asciitree-0-3-3/asciitree-0.3.3-py3-none-any.whl\n!pip install /kaggle/input/zarr-2-18-3/zarr-2.18.3-py3-none-any.whl\n!pip install /kaggle/input/ome-zarr-0-9-0/ome_zarr-0.9.0-py3-none-any.whl\n!pip install /kaggle/input/starfile-0-5-11-py3/starfile-0.5.11-py3-none-any.whl\n!pip install /kaggle/input/copick-0.8.1/copick-0.8.1-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:07.225127Z","iopub.execute_input":"2025-01-30T22:39:07.225334Z","iopub.status.idle":"2025-01-30T22:39:33.703311Z","shell.execute_reply.started":"2025-01-30T22:39:07.225303Z","shell.execute_reply":"2025-01-30T22:39:33.702174Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/asciitree-0-3-3/asciitree-0.3.3-py3-none-any.whl\nInstalling collected packages: asciitree\nSuccessfully installed asciitree-0.3.3\nProcessing /kaggle/input/zarr-2-18-3/zarr-2.18.3-py3-none-any.whl\nRequirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr==2.18.3) (0.3.3)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from zarr==2.18.3) (1.26.4)\nCollecting numcodecs>=0.10.0 (from zarr==2.18.3)\n  Downloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\nCollecting fasteners (from zarr==2.18.3)\n  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->zarr==2.18.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->zarr==2.18.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->zarr==2.18.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->zarr==2.18.3) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->zarr==2.18.3) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->zarr==2.18.3) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->zarr==2.18.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->zarr==2.18.3) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->zarr==2.18.3) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->zarr==2.18.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->zarr==2.18.3) (2024.2.0)\nDownloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\nInstalling collected packages: fasteners, numcodecs, zarr\nSuccessfully installed fasteners-0.19 numcodecs-0.13.1 zarr-2.18.3\nProcessing /kaggle/input/ome-zarr-0-9-0/ome_zarr-0.9.0-py3-none-any.whl\nRequirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from ome-zarr==0.9.0) (3.11.10)\nRequirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from ome-zarr==0.9.0) (2024.11.2)\nRequirement already satisfied: distributed in /usr/local/lib/python3.10/dist-packages (from ome-zarr==0.9.0) (2024.11.2)\nRequirement already satisfied: fsspec!=2021.07.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (2024.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ome-zarr==0.9.0) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ome-zarr==0.9.0) (2.32.3)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ome-zarr==0.9.0) (0.25.0)\nRequirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from ome-zarr==0.9.0) (0.12.1)\nRequirement already satisfied: zarr>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from ome-zarr==0.9.0) (2.18.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr==0.9.0) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr==0.9.0) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr==0.9.0) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr==0.9.0) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr==0.9.0) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr==0.9.0) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr==0.9.0) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr==0.9.0) (1.18.3)\nRequirement already satisfied: s3fs in /usr/local/lib/python3.10/dist-packages (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (2024.9.0)\nRequirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr>=2.8.1->ome-zarr==0.9.0) (0.3.3)\nRequirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr>=2.8.1->ome-zarr==0.9.0) (0.13.1)\nRequirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr>=2.8.1->ome-zarr==0.9.0) (0.19)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->ome-zarr==0.9.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->ome-zarr==0.9.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->ome-zarr==0.9.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->ome-zarr==0.9.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->ome-zarr==0.9.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->ome-zarr==0.9.0) (2.4.1)\nRequirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr==0.9.0) (8.1.7)\nRequirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr==0.9.0) (3.1.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr==0.9.0) (24.2)\nRequirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr==0.9.0) (1.4.2)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr==0.9.0) (6.0.2)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr==0.9.0) (8.5.0)\nRequirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (3.1.4)\nRequirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (1.0.0)\nRequirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (1.1.0)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (5.9.5)\nRequirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (2.4.0)\nRequirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (3.0.0)\nRequirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (6.3.3)\nRequirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (2.2.3)\nRequirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr==0.9.0) (3.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ome-zarr==0.9.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ome-zarr==0.9.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ome-zarr==0.9.0) (2024.12.14)\nRequirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ome-zarr==0.9.0) (1.13.1)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ome-zarr==0.9.0) (3.4.2)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ome-zarr==0.9.0) (11.0.0)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ome-zarr==0.9.0) (2.36.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ome-zarr==0.9.0) (2024.12.12)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ome-zarr==0.9.0) (0.4)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->ome-zarr==0.9.0) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed->ome-zarr==0.9.0) (3.0.2)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp<4->ome-zarr==0.9.0) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->ome-zarr==0.9.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->ome-zarr==0.9.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->ome-zarr==0.9.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->ome-zarr==0.9.0) (2024.2.0)\nRequirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (2.17.0)\nRequirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (0.12.0)\nRequirement already satisfied: botocore<1.35.94,>=1.35.74 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (1.35.93)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (2.8.2)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (1.0.1)\nRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (1.17.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->ome-zarr==0.9.0) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr==0.9.0) (1.17.0)\nInstalling collected packages: ome-zarr\nSuccessfully installed ome-zarr-0.9.0\nProcessing /kaggle/input/starfile-0-5-11-py3/starfile-0.5.11-py3-none-any.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from starfile==0.5.11) (1.26.4)\nRequirement already satisfied: pandas>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from starfile==0.5.11) (2.2.2)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from starfile==0.5.11) (17.0.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from starfile==0.5.11) (4.12.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.1->starfile==0.5.11) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.1->starfile==0.5.11) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.1->starfile==0.5.11) (2024.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->starfile==0.5.11) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->starfile==0.5.11) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->starfile==0.5.11) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->starfile==0.5.11) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->starfile==0.5.11) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->starfile==0.5.11) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.1->starfile==0.5.11) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->starfile==0.5.11) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->starfile==0.5.11) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->starfile==0.5.11) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->starfile==0.5.11) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->starfile==0.5.11) (2024.2.0)\nInstalling collected packages: starfile\nSuccessfully installed starfile-0.5.11\nRequirement already satisfied: starfile in /usr/local/lib/python3.10/dist-packages (0.5.11)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from starfile) (1.26.4)\nRequirement already satisfied: pandas>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from starfile) (2.2.2)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from starfile) (17.0.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from starfile) (4.12.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.1->starfile) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.1->starfile) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.1->starfile) (2024.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->starfile) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->starfile) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->starfile) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->starfile) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->starfile) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->starfile) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.1->starfile) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->starfile) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->starfile) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->starfile) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->starfile) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->starfile) (2024.2.0)\nCollecting copick\n  Downloading copick-0.8.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from copick) (8.1.7)\nCollecting cryoet-data-portal==4.0.0 (from copick)\n  Downloading cryoet_data_portal-4.0.0-py3-none-any.whl.metadata (2.0 kB)\nCollecting distinctipy (from copick)\n  Downloading distinctipy-1.3.4-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: fsspec>=2024.6.0 in /usr/local/lib/python3.10/dist-packages (from copick) (2024.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from copick) (1.26.4)\nRequirement already satisfied: ome-zarr in /usr/local/lib/python3.10/dist-packages (from copick) (0.9.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from copick) (5.9.5)\nRequirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from copick) (2.10.3)\nRequirement already satisfied: s3fs in /usr/local/lib/python3.10/dist-packages (from copick) (2024.9.0)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from copick) (0.25.0)\nCollecting trimesh (from copick)\n  Downloading trimesh-4.6.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: zarr<3 in /usr/local/lib/python3.10/dist-packages (from copick) (2.18.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from cryoet-data-portal==4.0.0->copick) (2.32.3)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from cryoet-data-portal==4.0.0->copick) (1.35.93)\nCollecting deepmerge (from cryoet-data-portal==4.0.0->copick)\n  Downloading deepmerge-2.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting gql[requests] (from cryoet-data-portal==4.0.0->copick)\n  Downloading gql-3.5.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cryoet-data-portal==4.0.0->copick) (4.67.1)\nCollecting strcase (from cryoet-data-portal==4.0.0->copick)\n  Downloading strcase-1.0.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->copick) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->copick) (2.27.1)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->copick) (4.12.2)\nRequirement already satisfied: asciitree in /usr/local/lib/python3.10/dist-packages (from zarr<3->copick) (0.3.3)\nRequirement already satisfied: numcodecs>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from zarr<3->copick) (0.13.1)\nRequirement already satisfied: fasteners in /usr/local/lib/python3.10/dist-packages (from zarr<3->copick) (0.19)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->copick) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->copick) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->copick) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->copick) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->copick) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->copick) (2.4.1)\nRequirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from ome-zarr->copick) (3.11.10)\nRequirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from ome-zarr->copick) (2024.11.2)\nRequirement already satisfied: distributed in /usr/local/lib/python3.10/dist-packages (from ome-zarr->copick) (2024.11.2)\nRequirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from ome-zarr->copick) (0.12.1)\nRequirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from s3fs->copick) (2.17.0)\nRequirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->copick) (1.13.1)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->copick) (3.4.2)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->copick) (11.0.0)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->copick) (2.36.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->copick) (2024.12.12)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->copick) (24.2)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->copick) (0.4)\nRequirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->copick) (0.12.0)\nRequirement already satisfied: botocore<1.35.94,>=1.35.74 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->copick) (1.35.93)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->copick) (2.8.2)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->copick) (1.0.1)\nRequirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->copick) (6.1.0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->copick) (2.2.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->copick) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr->copick) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr->copick) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr->copick) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr->copick) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr->copick) (1.5.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr->copick) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr->copick) (1.18.3)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->cryoet-data-portal==4.0.0->copick) (0.10.4)\nRequirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr->copick) (3.1.0)\nRequirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr->copick) (1.4.2)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr->copick) (6.0.2)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->ome-zarr->copick) (8.5.0)\nRequirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr->copick) (3.1.4)\nRequirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr->copick) (1.0.0)\nRequirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr->copick) (1.1.0)\nRequirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr->copick) (2.4.0)\nRequirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr->copick) (3.0.0)\nRequirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr->copick) (6.3.3)\nRequirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr->copick) (3.0.0)\nCollecting graphql-core<3.3,>=3.2 (from gql[requests]->cryoet-data-portal==4.0.0->copick)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting backoff<3.0,>=1.11.1 (from gql[requests]->cryoet-data-portal==4.0.0->copick)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gql[requests]->cryoet-data-portal==4.0.0->copick) (3.7.1)\nRequirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gql[requests]->cryoet-data-portal==4.0.0->copick) (1.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->cryoet-data-portal==4.0.0->copick) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->cryoet-data-portal==4.0.0->copick) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->cryoet-data-portal==4.0.0->copick) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->copick) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->copick) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->copick) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->copick) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.0->gql[requests]->cryoet-data-portal==4.0.0->copick) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.0->gql[requests]->cryoet-data-portal==4.0.0->copick) (1.2.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->ome-zarr->copick) (3.21.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->copick) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed->ome-zarr->copick) (3.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs->copick) (1.17.0)\nDownloading copick-0.8.1-py3-none-any.whl (35 kB)\nDownloading cryoet_data_portal-4.0.0-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading distinctipy-1.3.4-py3-none-any.whl (26 kB)\nDownloading trimesh-4.6.1-py3-none-any.whl (707 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m707.0/707.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading deepmerge-2.0-py3-none-any.whl (13 kB)\nDownloading strcase-1.0.0-py3-none-any.whl (3.2 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gql-3.5.0-py2.py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: strcase, graphql-core, deepmerge, backoff, gql, cryoet-data-portal, trimesh, distinctipy, copick\nSuccessfully installed backoff-2.2.1 copick-0.8.1 cryoet-data-portal-4.0.0 deepmerge-2.0 distinctipy-1.3.4 gql-3.5.0 graphql-core-3.2.6 strcase-1.0.0 trimesh-4.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# tools\nfrom scipy.spatial.transform import Rotation as R\nfrom typing import Any, Dict, List\n# import ome_zarr\nimport ome_zarr.writer\nimport starfile, zarr\nimport pandas as pd\nimport numpy as np\nimport json, os\nimport copick\n\n\ndef get_copick_project_tomoIDs(copickRoot):\n    copickRoot = copick.from_file(copickRoot)\n    tomoIDs = [run.name for run in copickRoot.runs]\n\n    return tomoIDs\n\ndef read_copick_tomogram_group(copickRoot, voxelSize, tomoAlgorithm, tomoID=None):\n    \"\"\"Find the Zarr Group Relating to a Copick Tomogram.\n\n    Args:\n        copickRoot: Target Copick Run to Extract Tomogram Zarrr Group.\n        voxelSize: Name of the Tomogram.\n        tomoAlgorithm: Session ID for the Tomogram.\n        tomoID: Tomogram ID for that Dataset.\n\n    Returns:\n        ZarrGroup for the Tomogram object.\n    \"\"\"\n\n    # Get First Run and Pull out Tomgram\n    run = copickRoot.get_run(copickRoot.runs[0].name) if tomoID is None else copickRoot.get_run(tomoID)\n\n    # Attempt to Retrieve Algorithm\n    try: \n        tomogram = run.get_voxel_spacing(voxelSize).get_tomogram(tomoAlgorithm)\n    except:\n        # Query Avaiable Voxel Spacings\n        availableVoxelSpacings = [tomo.voxel_size for tomo in run.voxel_spacings]\n\n        # Report to the user which spacings they can use \n        raise ValueError(f\"Voxel Spacings of '{voxelSize}' was not found. \"\n                         f\"Available spacings are: {', '.join(map(str, availableVoxelSpacings))}\")\n\n\n    # If Tomogram is Unavailable - Report the Correct Available Algorithms\n    if tomogram is None:\n\n        voxel_spacing = run.get_voxel_spacing(voxelSize)      \n\n        # Get available algorithms\n        availableAlgorithms = [tomo.tomo_type for tomo in run.get_voxel_spacing(voxelSize).tomograms]\n        \n        # Report to the user\n        raise ValueError(f\"The tomogram with the algorithm '{tomoAlgorithm}' was not found. \"\n                         f\"Available algorithms are: {', '.join(availableAlgorithms)}\")\n\n    # Convert Zarr into Vol and Extract Shape\n    group = zarr.open(tomogram.zarr())\n\n    return group\n\n\ndef get_copick_tomogram(copickRoot, voxelSize=10, tomoAlgorithm=\"denoised\", tomoID=None):\n    \"\"\"Return a Tomogram from a Copick Run.\n\n    Args:\n        copickRoot: Target Copick Root to Extract Tomogram.\n        voxelSize: Name of the Tomogram.\n        tomoAlgorithm: Session ID for the Tomogram.\n        tomoID: Tomogram ID for that Dataset.\n\n    Returns:\n        ZarrGroup for the Tomogram object.\n    \"\"\"\n\n    group = read_copick_tomogram_group(copickRoot, voxelSize, tomoAlgorithm, tomoID)\n\n    # Return Volume\n    return list(group.arrays())[0][1]\n\n\ndef get_copick_tomogram_shape(copickRoot, voxelSize=10, tomoAlgorithm=\"denoised\"):\n    \"\"\"Return a Tomogram Dimensions (nx, ny, nz) from a Copick Run.\n\n    Args:\n        copickRoot: Target Copick Run to Extract Tomogram Dimensions.\n        voxelSize: Name of the Tomogram.\n        tomoAlgorithm: Session ID for the Tomogram.\n        tomoID: Tomogram ID for that Dataset.\n\n    Returns:\n        TomogrameShape\n    \"\"\"\n\n    # Return Volume Shape\n    return get_copick_tomogram(copickRoot, voxelSize, tomoAlgorithm).shape\n\n\ndef get_target_empty_tomogram(copickRoot, voxelSize=10, tomoAlgorithm=\"denoised\"):\n    \"\"\"Return an Empty Tomogram with Equivalent Dimensions (nx, ny, nz) from a Copick Run.\n    Args:\n        copickRoot: Target Copick Run to Extract Empty Tomogram.\n        voxelSize: Name of the Tomogram.\n        tomoAlgorithm: Session ID for the Tomogram.\n\n    Returns:\n        A Tomogram Composed of Zeros with the Same Shape as in Copick Project.\n    \"\"\"\n\n    return np.zeros(get_copick_tomogram_shape(copickRoot, voxelSize, tomoAlgorithm), dtype=np.int8)\n\n\ndef get_copick_segmentation(copickRun, segmentationName=\"segmentationmap\", userID=\"deepfinder\", sessionID=\"1\"):\n    \"\"\"Return a Specified Copick Segmentation.\n    Args:\n        copickRun: Target Copick Run to Extract Tomogram.\n        segmentationName: Name of the Segmentation.\n        userID: User who Created the Segmentation.\n\n    Returns:\n        The Segmentation within that Copick-Run.\n    \"\"\"\n\n    # Get the Segmentation from the Following Copick Run\n    seg = copickRun.get_segmentations(name=segmentationName, user_id=userID, session_id=sessionID)[0]\n\n    # TODO: Add Query To List Out Available Segmentations if Incorrect SegmentationName, UserID, or SessionID is provided. \n\n    # Return the Corresponding Segmentation Volume\n    store = seg.zarr()\n\n    return zarr.open(store, mode=\"r\")[0]\n\n\ndef get_ground_truth_coordinates(copickRun, voxelSize, proteinName, userID = None, sessionID = None):\n    \"\"\"Get the Ground Truth Coordinates From Copick and Return as a Numpy Array.\n\n    Args:\n        copickRun: Voxel size for the segmentation.\n        voxelSize: Name of the segmentation.\n        proteinIndex: Session ID for the segmentation.\n\n    Returns:\n        coords: The newly created segmentation object.\n    \"\"\"\n\n    picks = copickRun.get_picks(proteinName,\n                                user_id = userID, \n                                session_id = sessionID)[0]\n\n    coords = []\n    for ii in range(len(picks.points)):\n        coords.append(\n            (\n                picks.points[ii].location.x / voxelSize,\n                picks.points[ii].location.y / voxelSize,\n                picks.points[ii].location.z / voxelSize,\n            ),\n        )\n\n    return np.array(coords)\n\ndef get_pickable_object_label(copickRoot, objectName):\n    for ii in range(len(copickRoot.pickable_objects)):\n        if copickRoot.pickable_objects[ii].name == objectName:\n            return copickRoot.pickable_objects[ii].label\n\n\ndef read_copick_json(filePath):\n    \"\"\"\n    Read and processes a copick JSON coordinate file and returns as NumPy array.\n\n    Args:\n    - filePath (str): The path to the JSON file to be read.\n\n    Returns:\n    - np.ndarray: A NumPy array where first three columns are the coordinates (x, y, z), and the next three columns are the Euler angles (in degrees).\n                  Note: X/Y/Z coordinates are returned in Angstroms.\n    \"\"\"\n\n    # Load JSON data from a file\n    with open(os.path.join(filePath), \"r\") as jFile:\n        data = json.load(jFile)\n\n    # Initialize lists to hold the converted data\n    coordinates = []\n    eulerAngles = []\n\n    # Loop through each point in the JSON data\n    for point in data[\"points\"]:\n        rotationMatrix = []\n\n        # Extract the location and convert it to a NumPy array\n        currLocation = np.array([point[\"location\"][\"x\"], point[\"location\"][\"y\"], point[\"location\"][\"z\"]])\n\n        # Extract the transformation matrix and convert it to a NumPy array\n        rotationMatrix = R.from_matrix(np.array(point[\"transformation_\"])[:3, :3])\n        currEulerAngles = rotationMatrix.as_euler(\"ZYZ\", degrees=True)\n\n        coordinates.append(currLocation)\n        eulerAngles.append(currEulerAngles)\n\n    return np.hstack((np.array(coordinates), np.array(eulerAngles)))\n\n\ndef convert_copick_coordinates_to_xml(copickRun, xml_objects, pixelSize=10):\n    picks = copickRun.picks\n    for _proteinLabel in range(len(picks)):\n        xml_objects.append()\n\n    return xml_objects\n\n\ndef write_relion_output(specimen, tomoID, coords, outputDirectory=\"refinedCoPicks/ExperimentRuns\", pixelSize=10):\n    \"\"\"\n    Read and processes a copick JSON coordinate file and returns as NumPy array.\n\n    Args:\n    - filePath (str): The path to the JSON file to be read.\n\n    Returns:\n    - np.ndarray: A NumPy array where first three columns are the coordinates (x, y, z), and the next three columns are the Euler angles (in degrees).\n                  Note: X/Y/Z coordinates are returned in Angstroms.\n    \"\"\"\n\n    outputStarFile = {}\n\n    # Coordinates\n    if coords.shape[0] > 0:\n        outputStarFile[\"rlnCoordinateX\"] = coords[:, 0] / pixelSize\n        outputStarFile[\"rlnCoordinateY\"] = coords[:, 1] / pixelSize\n        outputStarFile[\"rlnCoordinateZ\"] = coords[:, 2] / pixelSize\n\n        # Angles\n        outputStarFile[\"rlnAngleRot\"] = coords[:, 3]\n        outputStarFile[\"rlnAngleTilt\"] = coords[:, 4]\n        outputStarFile[\"rlnAnglePsi\"] = coords[:, 5]\n    else:\n        outputStarFile[\"rlnCoordinateX\"] = []\n        outputStarFile[\"rlnCoordinateY\"] = []\n        outputStarFile[\"rlnCoordinateZ\"] = []\n\n        # Angles\n        outputStarFile[\"rlnAngleRot\"] = []\n        outputStarFile[\"rlnAngleTilt\"] = []\n        outputStarFile[\"rlnAnglePsi\"] = []\n\n    # Write\n    if tomoID is None:\n        savePath = os.path.join(outputDirectory, f\"{specimen}.star\")\n    else:\n        savePath = os.path.join(outputDirectory, tomoID, f\"{tomoID}_{specimen}.star\")\n    starfile.write({\"particles\": pd.DataFrame(outputStarFile)}, savePath)\n\n\ndef write_copick_output(\n    specimen,\n    tomoID,\n    finalPicks,\n    outputDirectory=\"refinedCoPicks/ExperimentRuns\",\n    pickMethod=\"deepfindET\",\n    sessionID=\"0\",\n    knownTemplate=False,\n):\n    \"\"\"\n    Writes the output data from 3D protein picking algorithm into a JSON file.\n\n    Args:\n    - specimen (str): The name of the specimen.\n    - tomoID (str): The ID of the tomogram.\n    - finalPicks (np.ndarray): An array of final picks, where each row contains coordinates (x, y, z) and Euler angles (rotation, tilt, psi).\n    - outputDirectory (str): The directory where the output JSON file will be saved.\n    - pickMethod (str): The method used for picking. Default is 'deepfinder'.\n    - sessionID (str): The ID of the session. Default is '0'.\n    - knownTemplate (bool): A flag indicating whether the template is known. Default is False.\n    \"\"\"\n\n    # Define the JSON structure\n    json_data = {\n        \"pickable_object_name\": specimen,\n        \"user_id\": pickMethod,\n        \"session_id\": sessionID,\n        \"run_name\": tomoID,\n        \"voxel_spacing\": None,\n        \"unit\": \"angstrom\",\n    }\n    if not knownTemplate:\n        json_data[\"trust_orientation\"] = \"false\"\n\n    json_data[\"points\"] = []\n    for ii in range(finalPicks.shape[0]):\n        rotationMatrix = convert_euler_to_rotation_matrix(finalPicks[ii, 3], finalPicks[ii, 4], finalPicks[ii, 5])\n\n        # Append to points data\n        json_data[\"points\"].append(\n            {\n                \"location\": {\"x\": finalPicks[ii, 0], \"y\": finalPicks[ii, 1], \"z\": finalPicks[ii, 2]},\n                \"transformation_\": rotationMatrix,  # Convert matrix to list for JSON serialization\n                \"instance_id\": 0,\n                \"score\": 1.0,\n            },\n        )\n\n    # Generate custom formatted JSON\n    formatted_json = custom_format_json(json_data)\n\n    # Save to file\n    os.makedirs(os.path.join(outputDirectory, tomoID, \"Picks\"), exist_ok=True)\n    savePath = os.path.join(outputDirectory, tomoID, \"Picks\", f\"{pickMethod}_{sessionID}_{specimen}.json\")\n    with open(savePath, \"w\") as json_file:\n        json_file.write(formatted_json)\n\n\ndef custom_format_json(data):\n    result = \"{\\n\"\n    for key, value in data.items():\n        if key == \"points\":\n            result += '   \"{}\": [\\n'.format(key)\n            for point in value:\n                result += \"      {\\n\"\n                for p_key, p_value in point.items():\n                    if p_key in [\"location\", \"transformation_\"]:\n                        if p_key == \"location\":\n                            loc_str = \", \".join(['\"{}\": {}'.format(k, v) for k, v in p_value.items()])\n                            result += '         \"{}\": {{ {} }},\\n'.format(p_key, loc_str)\n                        if p_key == \"transformation_\":\n                            trans_str = \",\\n            \".join(\n                                [\"[{}]\".format(\", \".join(map(str, row))) for row in p_value],\n                            )\n                            result += '         \"{}\": [\\n            {}\\n         ],\\n'.format(p_key, trans_str)\n                    else:\n                        result += '         \"{}\": {},\\n'.format(p_key, json.dumps(p_value))\n                result = result.rstrip(\",\\n\") + \"\\n      },\\n\"\n            result = result.rstrip(\",\\n\") + \"\\n   ]\\n\"\n        else:\n            result += '   \"{}\": {},\\n'.format(key, json.dumps(value))\n    result = result.rstrip(\",\\n\") + \"\\n}\"\n    return result\n\n\ndef convert_euler_to_rotation_matrix(angleRot, angleTilt, anglePsi):\n    \"\"\"\n    Convert Euler angles (rotation, tilt, and psi) in the Relion 'ZYZ' convention into a 4x4 rotation matrix.\n\n    Parameters:\n    - angleRot (float): The rotation angle (in degrees) about the Z-axis.\n    - angleTilt (float): The tilt angle (in degrees) about the Y-axis.\n    - anglePsi (float): The psi angle (in degrees) about the Z-axis.\n\n    Returns:\n    - np.ndarray: Rotation matrix.\n    \"\"\"\n\n    rotation = R.from_euler(\"zyz\", [angleRot, angleTilt, anglePsi], degrees=True)\n    rotation_matrix = rotation.as_matrix()\n\n    # Append a zero column to the right and zero row at the bottom\n    new_column = np.zeros((3, 1))\n    new_row = np.zeros((1, 4))\n    rotation_matrix = np.vstack((np.hstack((rotation_matrix, new_column)), new_row))\n\n    # Set the last element to 1\n    rotation_matrix[-1, -1] = 1\n    rotation_matrix = np.round(rotation_matrix, 3)\n\n    return rotation_matrix\n\n\ndef ome_zarr_axes() -> List[Dict[str, str]]:\n    \"\"\"\n    Returns a list of dictionaries defining the axes information for an OME-Zarr dataset.\n\n    Returns:\n    - List[Dict[str, str]]: A list of dictionaries, each specifying the name, type, and unit of an axis.\n      The axes are 'z', 'y', and 'x', all of type 'space' and unit 'angstrom'.\n    \"\"\"\n    return [\n        {\n            \"name\": \"z\",\n            \"type\": \"space\",\n            \"unit\": \"angstrom\",\n        },\n        {\n            \"name\": \"y\",\n            \"type\": \"space\",\n            \"unit\": \"angstrom\",\n        },\n        {\n            \"name\": \"x\",\n            \"type\": \"space\",\n            \"unit\": \"angstrom\",\n        },\n    ]\n\n\ndef ome_zarr_transforms(voxel_size: float) -> List[Dict[str, Any]]:\n    \"\"\"\n    Return a list of dictionaries defining the coordinate transformations of OME-Zarr dataset.\n\n    Parameters:\n    - voxel_size (float): The size of a voxel.\n\n    Returns:\n    - List[Dict[str, Any]]: A list containing a single dictionary with the 'scale' transformation,\n      specifying the voxel size for each axis and the transformation type as 'scale'.\n    \"\"\"\n    return [{\"scale\": [voxel_size, voxel_size, voxel_size], \"type\": \"scale\"}]\n\ndef write_ome_zarr_tomogram(\n    run,\n    inputVol,\n    voxelSize=10,\n    tomo_algorithm=\"wbp\"\n    ):\n    \"\"\"\n    Write a OME-Zarr segmentation into a Copick Directory.\n\n    Parameters:\n    - run: The run object, which provides a method to create a new segmentation.\n    - segmentation: The segmentation data to be written.\n    - voxelsize (float): The size of the voxels. Default is 10.\n    \"\"\"\n\n    # Create a new segmentation or Read Previous Segmentation\n    vs = run.get_voxel_spacing(voxelSize)\n    if vs is None:\n        vs = run.new_voxel_spacing(\n            voxel_size=voxelSize,\n        )\n        tomogram = vs.new_tomogram(tomo_algorithm)\n    else:\n        tomogram = vs.get_tomogram(tomo_algorithm)\n        \n    # Write the zarr file\n    loc = tomogram.zarr()\n    root_group = zarr.group(loc, overwrite=True)\n\n    ome_zarr.writer.write_multiscale(\n        [inputVol],\n        group=root_group,\n        axes=ome_zarr_axes(),\n        coordinate_transformations=[ome_zarr_transforms(voxelSize)],\n        storage_options=dict(chunks=(256, 256, 256), overwrite=True),\n        compute=True,\n    )\n\ndef write_ome_zarr_segmentation(\n    run,\n    inputSegmentVol,\n    voxelSize=10,\n    segmentationName=\"segmentation\",\n    userID=\"deepfinder\",\n    sessionID=\"0\",\n    multilabel_seg = True\n    ):\n    \"\"\"\n    Write a OME-Zarr segmentation into a Copick Directory.\n\n    Parameters:\n    - run: The run object, which provides a method to create a new segmentation.\n    - segmentation: The segmentation data to be written.\n    - voxelsize (float): The size of the voxels. Default is 10.\n    \"\"\"\n\n    # Create a new segmentation or Read Previous Segmentation\n    seg = run.get_segmentations(name=segmentationName, user_id=userID, session_id=sessionID)\n\n    if len(seg) == 0 or seg[0].voxel_size != voxelSize:\n        seg = run.new_segmentation(\n            voxel_size=voxelSize,\n            name=segmentationName,\n            session_id=sessionID,\n            is_multilabel=multilabel_seg,\n            user_id=userID,\n        )\n    else:\n        seg = seg[0]\n\n\n    # Write the zarr file\n    loc = seg.zarr()\n    root_group = zarr.group(loc, overwrite=True)\n\n    ome_zarr.writer.write_multiscale(\n        [inputSegmentVol],\n        group=root_group,\n        axes=ome_zarr_axes(),\n        coordinate_transformations=[ome_zarr_transforms(voxelSize)],\n        storage_options=dict(chunks=(256, 256, 256), overwrite=True),\n        compute=True,\n    )\n\n\ndef ome_zarr_feature_axes() -> List[Dict[str, str]]:\n    \"\"\"\n    Returns a list of dictionaries defining the axes information for an OME-Zarr dataset.\n\n    Returns:\n    - List[Dict[str, str]]: A list of dictionaries, each specifying the name, type, and unit of an axis.\n      The axes are 'z', 'y', and 'x', all of type 'space' and unit 'angstrom'.\n    \"\"\"\n    return [\n        {\n            \"name\": \"c\",\n            \"type\": \"channel\",\n        },\n        {\n            \"name\": \"z\",\n            \"type\": \"space\",\n            \"unit\": \"angstrom\",\n        },\n        {\n            \"name\": \"y\",\n            \"type\": \"space\",\n            \"unit\": \"angstrom\",\n        },\n        {\n            \"name\": \"x\",\n            \"type\": \"space\",\n            \"unit\": \"angstrom\",\n        },\n    ]\n\n\ndef ome_zarr_feature_transforms(voxel_size: float) -> List[Dict[str, Any]]:\n    \"\"\"\n    Return a list of dictionaries defining the coordinate transformations of OME-Zarr dataset.\n\n    Parameters:\n    - voxel_size (float): The size of a voxel.\n\n    Returns:\n    - List[Dict[str, Any]]: A list containing a single dictionary with the 'scale' transformation,\n      specifying the voxel size for each axis and the transformation type as 'scale'.\n    \"\"\"\n    return [{\"scale\": [voxel_size, voxel_size, voxel_size, voxel_size], \"type\": \"scale\"}]\n\n\ndef write_ome_zarr_scoremap(\n    run,\n    inputScoreVol,\n    voxelSize=10,\n    scoremapName=\"scoremap\",\n    userID=\"deepfinder\",\n    sessionID=\"0\",\n    tomo_type=\"denoised\",\n):\n    \"\"\"\n    Write a OME-Zarr segmentation into a Copick Directory.\n\n    Parameters:\n    - run: The run object, which provides a method to create a new segmentation.\n    - segmentation: The segmentation data to be written.\n    - voxelsize (float): The size of the voxels. Default is 10.\n    - segmentationName (str): The name of the segmentation. Default is 'segmentation'.\n    - userID (str): The user ID. Default is 'deepfinder'.\n    - sessionID (str): The session ID. Default is '0'.\n    - tomo_type (str): The type of tomogram. Default is 'denoised'.\n    \"\"\"\n    inputScoreVol = np.transpose(inputScoreVol, (3, 0, 1, 2))\n    # Create a new segmentation or Read Previous Segmentation\n    tomo = run.get_voxel_spacing(voxelSize).get_tomogram(tomo_type)\n\n    feat = tomo.get_features(scoremapName)\n\n    if feat is None:\n        feat = tomo.new_features(\n            feature_type=scoremapName,\n        )\n\n    # Write the zarr file\n    loc = feat.zarr()\n    root_group = zarr.group(loc, overwrite=True)\n\n    ome_zarr.writer.write_multiscale(\n        [inputScoreVol],\n        group=root_group,\n        axes=ome_zarr_feature_axes(),\n        coordinate_transformations=[ome_zarr_feature_transforms(voxelSize)],\n        storage_options=dict(chunks=(1, 256, 256, 256), overwrite=True),\n        compute=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:06:29.354000Z","iopub.execute_input":"2025-01-30T22:06:29.354302Z","iopub.status.idle":"2025-01-30T22:06:34.775053Z","shell.execute_reply.started":"2025-01-30T22:06:29.354277Z","shell.execute_reply":"2025-01-30T22:06:34.774344Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(type(seg))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:06:35.005763Z","iopub.status.idle":"2025-01-30T22:06:35.006053Z","shell.execute_reply":"2025-01-30T22:06:35.005917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# res_unet\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D, UpSampling3D, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.layers import Input, concatenate, Add, Dropout\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\n\ndef residual_block(input, filters, kernel_size=(3, 3, 3)):\n    x = Conv3D(filters, kernel_size, padding='same')(input)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    x = Conv3D(filters, kernel_size, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    shortcut = input\n    if input.shape[-1] != filters:\n        shortcut = Conv3D(filters, (1, 1, 1), padding='same')(input)\n\n    x = Add()([shortcut, x])\n    x = LeakyReLU()(x)\n    return x\n\ndef my_res_unet_model(dim_in, Ncl, filters=[48, 64, 80], dropout_rate=0):\n\n    input = Input(shape=(dim_in, dim_in, dim_in, 1))\n\n    x = input\n    down_layers = []\n\n    # Encoder\n    for filter in filters[:-1]:\n        x = residual_block(x, filter)\n        if dropout_rate > 0: x = Dropout(dropout_rate)(x)\n        down_layers.append(x)\n        x = MaxPooling3D((2, 2, 2))(x)\n\n    # Bottleneck\n    for _ in range(4):\n        x = residual_block(x, filters[-1])\n\n    # Decoder\n    for filter, down_layer in zip(reversed(filters[:-1]), reversed(down_layers)):\n        x = UpSampling3D(size=(2, 2, 2))(x)\n        x = concatenate([x, down_layer])\n        x = residual_block(x, filter)\n        x = residual_block(x, filter)        \n        if dropout_rate > 0: x = Dropout(dropout_rate)(x)\n\n    output = Conv3D(Ncl, (1, 1, 1), padding='same', activation='softmax')(x)\n\n    model = Model(input, output)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:03:22.248891Z","iopub.execute_input":"2025-01-30T21:03:22.249625Z","iopub.status.idle":"2025-01-30T21:03:40.050711Z","shell.execute_reply.started":"2025-01-30T21:03:22.249591Z","shell.execute_reply":"2025-01-30T21:03:40.049731Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# unet\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D, UpSampling3D\nfrom tensorflow.keras.layers import Input, concatenate, Dropout\nfrom tensorflow.keras.models import Model\n\ndef conv_block(input, filters, kernel_size=(3,3,3), activation='relu'):\n    x = Conv3D(filters, kernel_size, padding='same', activation=activation)(input)\n    x = Conv3D(filters, kernel_size, padding='same', activation=activation)(x)\n    return x\n\ndef my_unet_model(dim_in, Ncl, filters=[32,48,64], dropout_rate=0):\n\n    input = Input(shape=(dim_in, dim_in, dim_in, 1))\n    \n    x = input\n    down_layers = []\n\n    # Encoder\n    for filter in filters[:-1]:\n        x = conv_block(x, filter)\n        x = Dropout(dropout_rate)(x)        \n        down_layers.append(x)\n        x = MaxPooling3D((2, 2, 2))(x)\n    \n    # Bottleneck\n    x = conv_block( conv_block(x, filters[-1] ), filters[-1] )\n    \n    # Decoder\n    for filter, down_layer in zip(reversed(filters[:-1]), reversed(down_layers)):\n        x = UpSampling3D(size=(2, 2, 2))(x)\n        x = Conv3D(filter, (2, 2, 2), padding='same', activation='relu')(x)\n        x = concatenate([x, down_layer])\n        x = conv_block(x, filter)\n\n    output = Conv3D(Ncl, (1, 1, 1), padding='same', activation='softmax')(x)\n    \n    model = Model(input, output)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:03:40.051871Z","iopub.execute_input":"2025-01-30T21:03:40.052663Z","iopub.status.idle":"2025-01-30T21:03:40.059224Z","shell.execute_reply.started":"2025-01-30T21:03:40.052635Z","shell.execute_reply":"2025-01-30T21:03:40.058565Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model_loader\n# from deepfindET.models.res_unet import my_res_unet_model\n# from deepfindET.models.unet import my_unet_model \n# from deepfindET import settings\nimport os\n\ndef load_model(dim_in, Ncl, model_name, trained_weights_path=None, filters = [48, 64, 128], dropout_rate = 0):\n\n    # Play with Model to Use\n    assert model_name in ['unet', 'res_unet'], \"Invalid model name specified. Use 'unet' or 'res_unet'.\"\n\n    if model_name == 'unet':\n        net = my_unet_model(dim_in, Ncl, filters, dropout_rate)\n    elif model_name == 'res_unet':\n        net = my_res_unet_model(dim_in, Ncl, filters, dropout_rate)\n    else:\n        raise ValueError(\"Invalid model name specified. Valid options {unet, or res_unet}\")\n\n    if trained_weights_path is not None:\n        if not os.path.exists(trained_weights_path):\n            raise FileNotFoundError(f\"The specified path for trained weights does not exist: {trained_weights_path}\")\n        net.load_weights(trained_weights_path)\n        print(f'\\nTraining {model_name} with {trained_weights_path} Weights\\n')\n    else:\n        print(f'\\nTraining {model_name} with Randomly Initialized Weights\\n')        \n\n    # Define model parameters\n    model_parameters = NetworkParameters(\n        architecture=model_name,\n        layers=filters,\n        dropout_rate=dropout_rate\n    )\n\n    return net, model_parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:03:40.060893Z","iopub.execute_input":"2025-01-30T21:03:40.061214Z","iopub.status.idle":"2025-01-30T21:03:40.100871Z","shell.execute_reply.started":"2025-01-30T21:03:40.061182Z","shell.execute_reply":"2025-01-30T21:03:40.100267Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# core\n# This script is adapted from a public GitHub repository.\n# Original source: https://github.com/deep-finder/cryoet-deepfinder/tree/master\n# Author: Inria,  Emmanuel Moebel, Charles Kervrann\n# License: GPL v3.0\n\n# from deepfindET.utils import copick_tools as copicktools\n# from deepfindET.utils import common as cm\nimport copick, h5py, os, sys\nfrom itertools import chain\nfrom tqdm import tqdm\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nmatplotlib.use(\"agg\")  # necessary else: AttributeError: 'NoneType' object has no attribute 'is_interactive'\n\nclass DeepFindET:\n    def __init__(self):\n        self.obs_list = [observer_print]\n\n    # Useful for sending prints to GUI\n    def set_observer(self, obs):\n        self.obs_list.append(obs)\n\n    # \"Master print\" calls all observers for prints\n    def display(self, message):\n        for obs in self.obs_list:\n            obs.display(message)\n\n    # For checking inputs:\n    def is_3D_nparray(self, v, varname):\n        if type(v) != np.ndarray:\n            self.display(\n                'DeepFindET message: variable \"' + varname + '\" is ' + str(type(v)) + \". Expected is numpy array.\",\n            )\n            sys.exit()\n        if len(v.shape) != 3:\n            self.display(\n                'DeepFindET message: variable \"'\n                + varname\n                + '\" is a '\n                + str(len(v.shape))\n                + \"D array. Expected is a 3D array.\",\n            )\n            sys.exit()\n\n    def is_int(self, v, varname):\n        if type(v) != int and type(v) != np.int8 and type(v) != np.int16:\n            self.display('DeepFindET message: variable \"' + varname + '\" is ' + str(type(v)) + \". Expected is int.\")\n            sys.exit()\n\n    def is_positive_int(self, v, varname):\n        self.is_int(v, varname)\n        if v <= 0:\n            self.display('DeepFindET message: variable \"' + varname + '\" is <=0. Expected is >0.')\n            sys.exit()\n\n    def is_multiple_4_int(self, v, varname):\n        self.is_int(v, varname)\n        if v % 4 != 0:\n            self.display('DeepFindET message: variable \"' + varname + '\" should be a multiple of 4.')\n            sys.exit()\n\n    def is_str(self, v, varname):\n        if type(v) != str:\n            self.display('DeepFindET message: variable \"' + varname + '\" is ' + str(type(v)) + \". Expected is str.\")\n            sys.exit()\n\n    def is_h5_path(self, v, varname):\n        self.is_str(v, varname)\n        s = os.path.splitext(v)\n        if s[1] != \".h5\":\n            self.display('DeepFindET message: \"' + str(varname) + '\" points to ' + s[1] + \", expected is .h5\")\n            sys.exit()\n\n    def is_list(self, v, varname):\n        if type(v) != list:\n            self.display('DeepFindET message: variable \"' + varname + '\" is ' + str(type(v)) + \". Expected is list.\")\n            sys.exit()\n\n    def check_array_minsize(self, v, varname):\n        lmin = v[1]  # is expected to be int (e.g. patch length)\n        if v[0].shape[0] < lmin and v[0].shape[1] < lmin and v[0].shape[2] < lmin:\n            self.display(\n                'DeepFindET message: the array \"'\n                + varname[0]\n                + '\" has shape '\n                + str(v[0].shape)\n                + '. Needs to be larger than array \"'\n                + varname[1]\n                + '\", which has shape ('\n                + str(v[1])\n                + \",\"\n                + str(v[1])\n                + \",\"\n                + str(v[1])\n                + \").\",\n            )\n            sys.exit()\n\n# Following observer classes are needed to send prints to GUI:\nclass observer_print:\n    def display(message):\n        print(message)\n\n# Retrieves variable name as a str:\n# Found here: https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\ndef retrieve_var_name(x, Vars=vars()):\n    for k in Vars:\n        if type(x) == type(Vars[k]) and x is Vars[k]:\n            return k\n    return None\n\n# This functions loads the training set at specified paths.\n# INPUTS:\n#   path_data  : list of strings '/path/to/tomogram.ext'\n#   path_target: list of strings '/path/to/target.ext'\n#                The idx of above lists correspond to each other so that (path_data[idx], path_target[idx]) corresponds\n#                to a (tomog, target) pair\n#   dset_name  : can be usefull if files are stored as .h5\n# OUTPUTS:\n#   data_list  : list of 3D numpy arrays (tomograms)\n#   target_list: list of 3D numpy arrays (annotated tomograms)\n#                In the same way as for the inputs, (data_list[idx],target_list[idx]) corresponds to a (tomo,target) pair\ndef load_copick_datasets(copickPath, train_instance, tomoIDs=None):\n    data_list = {}\n    target_list = {}\n\n    copickRoot = copick.from_file(copickPath)\n    if tomoIDs is None:\n        tomoIDs = [run.name for run in copickRoot.runs]\n\n    print(f\"Loading Targets and Tomograms for the Following Runs: {list(tomoIDs)}\")\n    for idx in tqdm(range(len(tomoIDs))):\n        target_list[tomoIDs[idx]] = copicktools.get_copick_segmentation(\n            copickRoot.get_run(tomoIDs[idx]),\n            train_instance.labelName,\n            train_instance.labelUserID,\n            train_instance.sessionID,\n        )[:]\n        data_list[tomoIDs[idx]] = copicktools.read_copick_tomogram_group(\n            copickRoot,\n            train_instance.voxelSize,\n            train_instance.tomoAlg,\n            tomoIDs[idx],\n        )[0][:]\n\n        if data_list[tomoIDs[idx]].shape != target_list[tomoIDs[idx]].shape:\n            print(f\"DeepFinder Message: tomogram and target for run {tomoIDs[idx]} are not of same size!\")\n            sys.exit()\n\n    return data_list, target_list\n\n\n# This function applies bootstrap (i.e. re-sampling) in case of unbalanced classes.\n# Given an objlist containing objects from various classes, this function outputs an equal amount of objects for each\n# class, each objects being uniformely sampled inside its class set.\n# INPUTS:\n#   objlist: list of dictionaries\n#   Nbs    : number of objects to sample from each class\n# OUTPUT:\n#   bs_idx : list of indexes corresponding to the bootstraped objects\ndef get_bootstrap_idx(objlist, Nbs):\n    # Get a list containing the object class labels (from objlist):\n    Nobj = len(objlist)\n    label_list = []\n    for idx in range(0, Nobj):\n        label_list.append(objlist[idx][\"label\"])\n\n    lblTAB = np.unique(label_list)  # vector containing unique class labels\n\n    # Bootstrap data so that we have equal frequencies (1/Nbs) for all classes:\n    # ->from label_list, sample Nbs objects from each class\n    bs_idx = []\n    for l in lblTAB:\n        bs_idx.append(np.random.choice(np.array(np.nonzero(np.array(label_list) == l))[0], Nbs))\n\n    bs_idx = np.concatenate(bs_idx)\n    return bs_idx\n\n\ndef query_available_picks(copickRoot, tomoIDs=None, targets=None):\n    # Load TomoIDs - Default is Read All TomoIDs from Path\n    if tomoIDs is None:\n        tomoIDs = [run.name for run in copickRoot.runs]\n\n    labelList = []\n    tomoIDList = []\n    pickIndList = []\n    proteinIndList = []\n    proteinCoordsList = []\n\n    for tomoInd in range(len(tomoIDs)):\n        copickRun = copickRoot.get_run(tomoIDs[tomoInd])\n\n        if targets is None:\n            query = copickRun.picks\n        else:\n            query = []\n            for target_name in targets:\n                query += copickRun.get_picks(\n                    object_name=target_name,\n                    user_id=targets[target_name][\"user_id\"],\n                    session_id=targets[target_name][\"session_id\"],\n                )\n\n        for proteinInd in range(len(query)):\n            picks = query[proteinInd]\n\n            nPicks = len(picks.points)\n            tomoIDList.append([tomoIDs[tomoInd]] * nPicks)\n            pickIndList.append(list(range(nPicks)))\n            proteinIndList.append([proteinInd] * nPicks)\n\n            proteinCoordsList.append(picks.points)\n            labelList.append([copicktools.get_pickable_object_label(copickRoot, picks.pickable_object_name)] * nPicks)\n\n    labelList = np.array(list(chain.from_iterable(labelList)))\n    tomoIDList = np.array(list(chain.from_iterable(tomoIDList)))\n    pickIndList = np.array(list(chain.from_iterable(pickIndList)))\n    proteinIndList = np.array(list(chain.from_iterable(proteinIndList)))\n    proteinCoordsList = np.array(list(chain.from_iterable(proteinCoordsList)))\n\n    return {\n        \"labelList\": labelList,\n        \"tomoIDlist\": tomoIDList,\n        \"pickIndList\": pickIndList,\n        \"proteinIndList\": proteinIndList,\n        \"proteinCoordsList\": proteinCoordsList,\n    }\n\n\ndef get_copick_boostrap_idx(organizedPicksDict, Nbs):\n    # Bootstrap data so that we have equal frequencies (1/Nbs) for all classes:\n    # ->from label_list, sample Nbs objects from each class\n    bs_idx = []\n    tomoID_idx = []\n    pick_idx = []\n    protein_idx = []\n    protein_picks = []\n    lblTAB = np.unique(organizedPicksDict[\"labelList\"])  # vector containing unique class labels\n    for l in lblTAB:\n        bsIndex = np.random.choice(np.array(np.nonzero(organizedPicksDict[\"labelList\"] == l))[0], Nbs)\n\n        bs_idx.append(bsIndex)\n        pick_idx.append(organizedPicksDict[\"pickIndList\"][bsIndex])\n        tomoID_idx.append(organizedPicksDict[\"tomoIDlist\"][bsIndex])\n        protein_idx.append(organizedPicksDict[\"proteinIndList\"][bsIndex])\n        protein_picks.append(organizedPicksDict[\"proteinCoordsList\"][bsIndex])\n\n    return {\n        \"bs_idx\": np.concatenate(bs_idx),\n        \"tomoID_idx\": np.concatenate(tomoID_idx),\n        \"protein_idx\": np.concatenate(protein_idx),\n        \"pick_idx\": np.concatenate(pick_idx),\n        \"protein_coords\": np.concatenate(protein_picks),\n    }\n\n\n# Takes position specified in 'obj', applies random shift to it, and then checks if the patch around this position is\n# out of the tomogram boundaries. If so, the position is shifted to that patch is inside the tomo boundaries.\n# INPUTS:\n#   tomodim: tuple (dimX,dimY,dimZ) containing size of tomogram\n#   p_in   : int lenght of patch in voxels\n#   obj    : dictionary obtained when calling objlist[idx]\n#   Lrnd   : int random shift in voxels applied to position\n# OUTPUTS:\n#   x,y,z  : int,int,int coordinates for sampling patch safely\ndef get_copick_patch_position(tomodim, p_in, Lrnd, voxelSize, copicks):\n    # sample at coordinates specified in obj=objlist[idx]\n    x = int(copicks.location.x / voxelSize)\n    y = int(copicks.location.y / voxelSize)\n    z = int(copicks.location.z / voxelSize)\n\n    x,y,z = add_random_shift(tomodim,p_in,Lrnd,x,y,z)\n\n    return x, y, z\n\ndef add_random_shift(tomodim, p_in, Lrnd, x,y,z):\n\n    # Add random shift to coordinates:\n    x = x + np.random.choice(range(-Lrnd,Lrnd+1))\n    y = y + np.random.choice(range(-Lrnd,Lrnd+1))\n    z = z + np.random.choice(range(-Lrnd,Lrnd+1))\n    \n    # Shift position if too close to border:\n    if (x<p_in) : x = p_in\n    if (y<p_in) : y = p_in\n    if (z<p_in) : z = p_in\n    if (x>tomodim[2]-p_in): x = tomodim[2]-p_in\n    if (y>tomodim[1]-p_in): y = tomodim[1]-p_in\n    if (z>tomodim[0]-p_in): z = tomodim[0]-p_in\n\n    #else: # sample random position in tomogram\n    #    x = np.int32( np.random.choice(range(p_in,tomodim[0]-p_in)) )\n    #    y = np.int32( np.random.choice(range(p_in,tomodim[0]-p_in)) )\n    #    z = np.int32( np.random.choice(range(p_in,tomodim[0]-p_in)) )\n    \n    return x,y,z \n\n\n# Saves training history as .h5 file.\n# INPUTS:\n#   history: dictionary object containing lists. These lists contain scores and metrics wrt epochs.\n#   filename: string '/path/to/net_train_history.h5'\ndef save_history(history, filename):\n    if os.path.isfile(filename):  # if file exists, delete before writing the updated version\n        os.remove(filename)  # quick fix for OSError: Can't write data (no appropriate function for conversion path)\n\n    h5file = h5py.File(filename, \"w\")\n\n    # train and val loss & accuracy:\n    dset = h5file.create_dataset(\"acc\", np.array(history[\"acc\"]).shape, dtype=\"float16\")\n    dset[:] = np.array(history[\"acc\"], dtype=\"float16\")\n    dset = h5file.create_dataset(\"loss\", np.array(history[\"loss\"]).shape, dtype=\"float16\")\n    dset[:] = np.array(history[\"loss\"], dtype=\"float16\")\n    dset = h5file.create_dataset(\"val_acc\", np.array(history[\"val_acc\"]).shape, dtype=\"float16\")\n    dset[:] = np.array(history[\"val_acc\"], dtype=\"float16\")\n    dset = h5file.create_dataset(\"val_loss\", np.array(history[\"val_loss\"]).shape, dtype=\"float16\")\n    dset[:] = np.array(history[\"val_loss\"], dtype=\"float16\")\n\n    # val precision, recall, F1:\n    dset = h5file.create_dataset(\"val_f1\", np.array(history[\"val_f1\"]).shape, dtype=\"float16\")\n    dset[:] = np.array(history[\"val_f1\"], dtype=\"float16\")\n    dset = h5file.create_dataset(\"val_precision\", np.array(history[\"val_precision\"]).shape, dtype=\"float16\")\n    dset[:] = np.array(history[\"val_precision\"], dtype=\"float16\")\n    dset = h5file.create_dataset(\"val_recall\", np.array(history[\"val_recall\"]).shape, dtype=\"float16\")\n    dset[:] = np.array(history[\"val_recall\"], dtype=\"float16\")\n\n    h5file.close()\n    return\n\n\ndef read_history(filename, save_fig=True):\n    history = {\n        \"acc\": None,\n        \"loss\": None,\n        \"val_acc\": None,\n        \"val_loss\": None,\n        \"val_f1\": None,\n        \"val_precision\": None,\n        \"val_recall\": None,\n    }\n\n    h5file = h5py.File(filename, \"r\")\n    # train and val loss & accuracy:\n    history[\"acc\"] = h5file[\"acc\"][:]\n    history[\"loss\"] = h5file[\"loss\"][:]\n    history[\"val_acc\"] = h5file[\"val_acc\"][:]\n    history[\"val_loss\"] = h5file[\"val_loss\"][:]\n    # val precision, recall, F1:\n    history[\"val_f1\"] = h5file[\"val_f1\"][:]\n    history[\"val_precision\"] = h5file[\"val_precision\"][:]\n    history[\"val_recall\"] = h5file[\"val_recall\"][:]\n\n    h5file.close()\n    return history\n\n\n# Plots the training history as several graphs and saves them in an image file.\n# Validation score is averaged over all batches tested in validation step (steps_per_valid)\n# Training score is averaged over last N=steps_per_valid batches of each epoch.\n#   -> This is to have similar curve smoothness to validation.\n# INPUTS:\n#   history: dictionary object containing lists. These lists contain scores and metrics wrt epochs.\n#   filename: string '/path/to/net_train_history_plot.png'\ndef plot_history(history, \n                filename:str = 'net_train_history.png', \n                save_figure: bool = True):\n\n    Ncl = len(history[\"val_f1\"][0])\n    legend_names = []\n    for lbl in range(0, Ncl):\n        legend_names.append(\"class \" + str(lbl))\n\n    len(history[\"val_loss\"])\n\n    hist_loss_train = history[\"loss\"]\n    hist_acc_train = history[\"acc\"]\n    hist_loss_valid = history[\"val_loss\"]\n    hist_acc_valid = history[\"val_acc\"]\n    hist_f1 = history[\"val_f1\"]\n    hist_recall = history[\"val_recall\"]\n    hist_precision = history[\"val_precision\"]\n\n    fig = plt.figure(figsize=(15, 12))\n    plt.subplot(321)\n    plt.plot(hist_loss_train, label=\"train\")\n    plt.plot(hist_loss_valid, label=\"valid\")\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epochs\")\n    plt.legend()\n    plt.grid()\n\n    plt.subplot(323)\n    plt.plot(hist_acc_train, label=\"train\")\n    plt.plot(hist_acc_valid, label=\"valid\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epochs\")\n    plt.legend()\n    plt.grid()\n\n    plt.subplot(322)\n    plt.plot(hist_f1)\n    plt.ylabel(\"F1-score\")\n    plt.xlabel(\"epochs\")\n    plt.legend(legend_names)\n    plt.grid()\n\n    plt.subplot(324)\n    plt.plot(hist_precision)\n    plt.ylabel(\"precision\")\n    plt.xlabel(\"epochs\")\n    plt.grid()\n\n    plt.subplot(326)\n    plt.plot(hist_recall)\n    plt.ylabel(\"recall\")\n    plt.xlabel(\"epochs\")\n    plt.grid()\n\n    if save_figure:\n        fig.savefig(filename)\n\ndef convert_hdf5_to_dictionary(filename: str):\n    \"\"\"\n    Converts an HDF5 file into a nested dictionary. Each group in the HDF5 file becomes a \n    nested dictionary, and datasets are converted to NumPy arrays.\n\n    Parameters:\n    filename (str): Path to the HDF5 file to be converted.\n\n    Returns:\n    dict: A dictionary representation of the HDF5 file contents.\n    \"\"\"\n\n    with h5py.File(filename, 'r') as hdf:\n        def recursively_load_dict_contents(hdf_group):\n            \"\"\"Recursively loads the HDF5 group into a nested dictionary.\"\"\"\n            ans = {}\n            for key, item in hdf_group.items():\n                # Check if the item is a dataset and convert it to a NumPy array\n                if isinstance(item, h5py.Dataset):\n                    ans[key] = item[()]  # Get the dataset as a NumPy array\n                # If the item is another group, recurse into it and convert it to a dictionary\n                elif isinstance(item, h5py.Group):\n                    ans[key] = recursively_load_dict_contents(item)\n            return ans\n        \n        # Convert the HDF5 file contents into a dictionary\n        history_dict = recursively_load_dict_contents(hdf)\n\n    # Return the fully constructed dictionary\n    return history_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:03:40.101727Z","iopub.execute_input":"2025-01-30T21:03:40.101916Z","iopub.status.idle":"2025-01-30T21:03:40.137869Z","shell.execute_reply.started":"2025-01-30T21:03:40.101900Z","shell.execute_reply":"2025-01-30T21:03:40.137168Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List, Dict, Optional\nfrom pydantic import BaseModel\nimport os, json\n\nclass ProcessingInput(BaseModel):\n    config_path_train: str\n    config_path_valid: Optional[str]\n    target_name: str\n    target_user_id: str\n    target_session_id: str\n\nclass ProcessingOutput(BaseModel):\n    out_dir: str    \n    classes: Dict[str, int]\n\nclass TrainingParameters(BaseModel):\n    n_class: int\n    dim_in: int\n    batch_size: int\n    epochs: int\n    steps_per_epoch: int\n    steps_per_valid: int\n    num_sub_epoch: int\n    sample_size: int\n    loss: str\n    class_weights: Dict[str, int]\n\nclass LearningRateParameters(BaseModel):\n    learning_rate: float    \n    min_learning_rate: float\n    monitor: str\n    factor: float\n    patience: int\n\nclass NetworkParameters(BaseModel):\n    architecture: str\n    layers: List[int]\n    dropout_rate: float\n    # activation_function: str\n\nclass ExperimentConfig(BaseModel):\n    input: ProcessingInput\n    output: ProcessingOutput\n    network_architecture: NetworkParameters\n    training_params: TrainingParameters\n    learning_params: LearningRateParameters\n\n    def save_to_json(self):\n        # Ensure the output directory exists\n        os.makedirs(self.output.out_dir, exist_ok=True)\n\n        # Define the path to the JSON file\n        json_file_path = os.path.join(self.output.out_dir, \"experiment_config.json\")\n\n        # Save the combined parameters to a JSON file\n        with open(json_file_path, 'w') as f:\n            json.dump(self.dict(), f, indent=4)\n\n        print(f\"Configuration saved to {json_file_path}\")    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:03:40.138679Z","iopub.execute_input":"2025-01-30T21:03:40.138992Z","iopub.status.idle":"2025-01-30T21:03:40.170767Z","shell.execute_reply.started":"2025-01-30T21:03:40.138964Z","shell.execute_reply":"2025-01-30T21:03:40.170183Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# step3\n# import deepfindET.utils.copick_tools as tools\n# from deepfindET.inference import Segment\nimport click, copick, json\nimport tensorflow as tf\nfrom typing import List\n\n############################\n# if \"-f\" in sys.argv:\n#     sys.argv.remove(\"-f\")\n############################\n\ndef parse_filters(ctx, param, value):\n    try:\n        # Split the comma-separated string and convert each part to an integer\n        filters = [int(x) for x in value.split(\",\")]\n        return filters\n    except ValueError:\n        raise click.BadParameter(\"Filters must be a comma-separated list of integers.\")\n\n@click.group()\n@click.pass_context\ndef cli(ctx):\n    pass\n\n@cli.command()\n@click.option(\n    \"--predict-config\",\n    type=str,\n    required=True,\n    help=\"Path to the copick config file.\",\n)\n@click.option(\n    \"--model-name\",\n    type=str,\n    required=False,\n    default=\"res_unet\",\n    show_default=True,\n    help=\"Model Architecture Name to Load For Inference\",\n)\n@click.option(\n    \"--path-weights\",\n    type=str,\n    required=True,\n    help=\"Path to the Trained Model Weights.\",\n)\n@click.option(\n    \"--model-filters\",\n    type=str,\n    required=False,\n    default=\"48,64,80\",\n    show_default=True,\n    callback=parse_filters,\n    help=\"Comma-separated list of filters for the model architecture.\",\n)\n@click.option(\n    \"--model-dropout\",\n    type=float,\n    required=False,\n    default=0,\n    show_default=True,\n    help=\"Dropout for Model Architecture.\",\n)\n@click.option(\n    \"--n-class\",\n    type=int,\n    required=True,\n    help=\"Number of classes.\",\n)\n@click.option(\n    \"--patch-size\",\n    type=int,\n    required=True,\n    help=\"Patch of Volume for Input to Network.\",\n)\n@click.option(\n    \"--user-id\", \n    type=str, \n    default=\"deepfindET\", \n    show_default=True, \n    required=False,\n    help=\"User ID filter for input.\"\n)\n@click.option(\n    \"--session-id\", \n    type=str, \n    default=None, \n    show_default=True, \n    required=True,\n    help=\"Session ID filter for input.\"\n)\n@click.option(\n    \"--voxel-size\",\n    type=float,\n    required=False,\n    default=10.0,\n    show_default=True,\n    help=\"Voxel size of the tomograms to segment.\",\n)\n@click.option(\n    \"--tomogram-algorithm\",\n    type=str,\n    required=False,\n    default=\"wbp\",\n    show_default=True,\n    help=\"Tomogram Algorithm.\",\n)\n@click.option(\n    \"--parallel-mpi/--no-parallel-mpi\",\n    default=False,\n    required=False,\n    show_default=True,\n    help=\"Patch of Volume for Input to Network.\",\n)\n@click.option(\n    \"--tomo-ids\",\n    type=str,\n    required=False,\n    default=None,\n    show_default=True,\n    help=\"Tomogram IDs to Segment.\",\n)\n@click.option(\n    \"--output-scoremap\",\n    type=bool,\n    required=False,\n    default=False,\n    show_default=True,\n    help=\"Output scoremap.\",\n)\n@click.option(\n    \"--scoremap-name\",\n    type=str,\n    required=False,\n    default=\"scoremap\",\n    show_default=True,\n    help=\"Output name for scoremap.\",\n)\n@click.option(\n    \"--segmentation-name\",\n    type=str,\n    required=False,\n    default=\"segmentation\",\n    show_default=True,\n    help=\"Output name for segmentation.\",\n)\ndef segment(\n    predict_config: str,\n    n_class: int,\n    model_name: str,\n    path_weights: str,\n    model_filters: List[int],\n    model_dropout: float,\n    patch_size: int,\n    user_id: str,\n    session_id: str,\n    voxel_size: float = 10,\n    tomogram_algorithm: str = \"denoised\",\n    parallel_mpi: bool = False,\n    tomo_ids: str = None,\n    output_scoremap: bool = False,\n    scoremap_name: str = \"scoremap\",\n    segmentation_name: str = \"segmentation\",\n):\n\n    inference_tomogram_segmentation(predict_config, n_class, model_name, path_weights, patch_size, \n                                    user_id, session_id, voxel_size, model_filters, model_dropout, \n                                    tomogram_algorithm, parallel_mpi, tomo_ids, output_scoremap, \n                                    scoremap_name, segmentation_name)\n\ndef inference_tomogram_segmentation(\n    predict_config: str,\n    n_class: int,\n    model_name: str,\n    path_weights: str,\n    patch_size: int,\n    user_id: str,\n    session_id: str,\n    voxel_size: float = 10,\n    model_filters: List[int] = [48, 64, 80],\n    model_dropout: float = 0,\n    tomogram_algorithm: str = \"denoised\",\n    parallel_mpi: bool = False,\n    tomo_ids: str = None,\n    output_scoremap: bool = False,\n    scoremap_name: str = \"scoremap\",\n    segmentation_name: str = \"segmentation\",        \n    ):\n\n    # Determine if Using MPI or Sequential Processing\n    if parallel_mpi:\n        from mpi4py import MPI\n\n        # Initialize MPI (Get Rank and nProc)\n        comm = MPI.COMM_WORLD\n        rank = comm.Get_rank()\n        nProcess = comm.Get_size()\n\n        locGPU = rank % len(tf.config.list_physical_devices('GPU'))\n    else:\n        nProcess = 1\n        rank = 0\n        locGPU = None\n\n    ############## (Step 1) Initialize segmentation task: ##############\n\n    # Load CoPick root\n    copickRoot = copick.from_file(predict_config)\n\n    seg = Segment(n_class, model_name, path_weights=path_weights, \n                  patch_size=patch_size, model_filters = model_filters, model_dropout = model_dropout, \n                  gpuID = locGPU)\n\n    # Load Evaluate TomoIDs\n    evalTomos = tomo_ids.split(\",\") if tomo_ids is not None else [run.name for run in copickRoot.runs]\n\n    # Print Segmenation Parameters\n    store_segmentation_parameters(predict_config, n_class, model_name, path_weights, patch_size, user_id,\n                                  session_id, voxel_size, model_filters, model_dropout, tomogram_algorithm,\n                                  tomo_ids, output_scoremap, scoremap_name, segmentation_name)    \n\n    # Create Temporary Empty Folder\n    for tomoInd in range(len(evalTomos)):\n        if (tomoInd + 1) % nProcess == rank:\n            # Extract TomoID and Associated Run\n            tomoID = evalTomos[tomoInd]\n            print(f'\\nProcessing Run: {tomoID} ({tomoInd}/{len(evalTomos)})')\n\n            # Load data:\n            tomo = get_copick_tomogram(\n                copickRoot,\n                voxelSize=voxel_size,\n                tomoAlgorithm=tomogram_algorithm,\n                tomoID=tomoID,\n            )\n\n            # Segment tomogram:\n            scoremaps = seg.launch(tomo[:])\n\n            # Query Copick Runs\n            copickRun = copickRoot.get_run(tomoID)\n\n            # Write scoremaps to file:\n            if output_scoremap:\n                write_ome_zarr_scoremap(\n                    copickRun,\n                    scoremaps,\n                    voxelSize=voxel_size,\n                    scoremapName=scoremap_name,\n                    userID=user_id,\n                    sessionID=session_id,\n                    tomo_type=tomogram_algorithm,\n                )\n\n            # Get labelmap from scoremaps:\n            labelmap = seg.to_labelmap(scoremaps)\n            write_ome_zarr_segmentation(\n                copickRun,\n                labelmap,\n                voxelSize=voxel_size,\n                segmentationName=segmentation_name,\n                userID=user_id,\n                sessionID=session_id,\n            )\n\n    print(\"Segmentations Complete!\")\n\ndef store_segmentation_parameters(predict_config: str,\n    n_class: int,\n    model_name: str,\n    path_weights: str,\n    patch_size: int,\n    user_id: str,\n    session_id: str,\n    voxel_size: float = 10,\n    model_filters: List[int] = [48, 64, 80],\n    model_dropout: float = 0,\n    tomogram_algorithm: str = \"denoised\",\n    tomo_ids: str = None,\n    output_scoremap: bool = False,\n    scoremap_name: str = \"scoremap\",\n    segmentation_name: str = \"segmentation\"):\n\n    parameters = {\n        \"input\": {\n            \"predict_config\": predict_config,\n            \"voxel_size\": voxel_size,\n            \"tomogram_algorithm\": tomogram_algorithm\n        },\n        \"model_architecture\": {\n            \"n_class\": n_class,\n            \"model_name\": model_name,\n            \"path_weights\": path_weights,\n            \"patch_size\": patch_size,\n            \"model_filters\": model_filters,\n            \"model_dropout\": model_dropout\n        },\n        \"output\": {\n            \"user_id\": user_id,\n            \"session_id\": session_id,\n            \"output_scoremap\": output_scoremap,\n            \"scoremap_name\": scoremap_name,\n            \"segmentation_name\": segmentation_name,\n            \"tomo_ids\": tomo_ids\n        }\n    }\n    print('\\nSegmentation Parameters: ', json.dumps(parameters,indent=4),'\\n')\n\n    # Save to JSON file\n    output_file = f'{user_id}_{session_id}_{segmentation_name}_seg_params.json'\n    with open(output_file, 'w') as json_file:\n        json.dump(parameters, json_file, indent=4)\n\n# if __name__ == \"__main__\": ###############\nif __name__ == \"__main__\" and not hasattr(sys, 'ps1'):\n    cli()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:03:44.091919Z","iopub.execute_input":"2025-01-30T21:03:44.092224Z","iopub.status.idle":"2025-01-30T21:03:44.136942Z","shell.execute_reply.started":"2025-01-30T21:03:44.092202Z","shell.execute_reply":"2025-01-30T21:03:44.136122Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# step4\n# import deepfindET.utils.copick_tools as tools\n# import deepfindET.utils.evaluate as eval\nimport scipy.ndimage as ndimage\nimport click, copick, os, json\nfrom tqdm import tqdm \nimport numpy as np\n\n@click.group()\n@click.pass_context\ndef cli(ctx):\n    pass\n\n@cli.command()\n@click.option(\n    \"--predict-config\",\n    type=str,\n    required=True,\n    help=\"Path to the copick config file.\",\n)\n@click.option(\n    \"--n-class\",\n    type=int,\n    required=True,\n    help=\"Number of classes.\",\n)\n@click.option(\n    \"--user-id\", \n    type=str, \n    default=\"deepfindET\", \n    show_default=True, \n    help=\"User ID filter for input.\"\n)\n@click.option(\n    \"--picks-session-id\", \n    type=str, \n    default=None, \n    show_default=True, \n    help=\"Session ID filter for input.\"\n)\n@click.option(\n    \"--segmentation-session-id\", \n    type=str, \n    default=1, \n    show_default=True, \n    help=\"Session ID filter for input.\"\n)\n@click.option(\n    \"--segmentation-name\",\n    type=str,\n    required=False,\n    default=\"segmentation\",\n    show_default=True,\n    help=\"Name for segmentation prediction.\",    \n)\n@click.option(\n    \"--voxel-size\",\n    type=float,\n    required=False,\n    default=10.0,\n    show_default=True,\n    help=\"Voxel size of the tomograms to segment.\",\n)\n@click.option(\n    \"--parallel-mpi/--no-parallel-mpi\",\n    default=False,\n    help=\"Patch of Volume for Input to Network.\",\n)\n@click.option(\n    \"--tomo-ids\",\n    type=str,\n    required=False,\n    default=None,\n    help=\"Tomogram IDs to Segment.\",\n)\n@click.option(\n    \"--path-output\",\n    type=str,\n    required=False,\n    default=\"deefinder_predictions/ExperimentRuns\",\n    help=\"Path to Copick Project to Write Results\"\n)\n@click.option(\n    \"--starfile-write-path\",\n    type=str,\n    required=False,\n    default=None,\n    help=\"Write Path to Save Starfile Coordinate Files Per Protein.\",\n)\n@click.option(\n    \"--min-protein-size\",\n    type=float,\n    required=False,\n    default=0.8,\n    help=\"Specifies the minimum size of protein objects to be considered during the localization process. \"\n          \"This parameter helps filter out small false positives objects based on their size. The value should be between (0, 1.0], \"\n          \"representing a fraction of the typical protein size defined in the configuration. Objects smaller than this threshold will be ignored. \",\n)\ndef localize(\n    predict_config: str,\n    n_class: int,\n    user_id: str, \n    picks_session_id: str,\n    segmentation_session_id: str,    \n    segmentation_name: str,\n    voxel_size: float,\n    parallel_mpi: bool = False,\n    tomo_ids: str = None,\n    path_output: str = \"deefinder_predictions/ExperimentRuns\",\n    starfile_write_path: str = None,\n    min_protein_size: float = 0.8, \n    ):\n\n    convert_segmentations_to_coordinates(predict_config, n_class, user_id, picks_session_id, \n                                         segmentation_session_id, segmentation_name, voxel_size, parallel_mpi, tomo_ids, path_output, \n                                         starfile_write_path, min_protein_size)\n\n\ndef convert_segmentations_to_coordinates(\n    predict_config: str,\n    n_class: int,\n    user_id: str, \n    picks_session_id: str,\n    segmentation_session_id: str,    \n    segmentation_name: str,\n    voxel_size: float,\n    parallel_mpi: bool = False,\n    tomo_ids: str = None,\n    path_output: str = \"deepfinder_predictions/ExperimentRuns\",\n    starfile_write_path: str = None,\n    min_protein_size: float = 0.8, \n    ):\n\n    # Determine if Using MPI or Sequential Processing\n    if parallel_mpi:\n        from mpi4py import MPI\n\n        # Initialize MPI (Get Rank and nProc)\n        comm = MPI.COMM_WORLD\n        rank = comm.Get_rank()\n        nProcess = comm.Get_size()\n    else:\n        nProcess = 1\n        rank = 0\n\n    # Print / Save Localization Parameters\n    store_localization_parameters(predict_config, n_class, user_id, picks_session_id, \n                                  segmentation_session_id, segmentation_name, voxel_size,\n                                  tomo_ids, path_output, starfile_write_path, min_protein_size)\n\n    # Open and read the Config File\n    with open(predict_config, 'r') as file:\n        data = json.load(file)\n    \n    # Get Path Output from Config File\n    # path_output = os.path.join(data['overlay_root'], 'ExperimentRuns')\n\n    # Create dictionary with name as key and diameter as value\n    proteins = {obj['name']: (obj['radius'], obj['label']) for obj in data['pickable_objects']}\n\n    # Create a reverse dictionary with label as key and name as value\n    label_to_name_dict = {obj['label']: obj['name'] for obj in data['pickable_objects']}          \n\n    # Load CoPick root\n    copickRoot = copick.from_file(predict_config)\n\n    # Load Evaluate TomoIDs\n    evalTomos = tomo_ids.split(\",\") if tomo_ids is not None else [run.name for run in copickRoot.runs]\n\n    # Currently Filtering Process always finds coordinate at (cx,cy,cz) - center coordinate\n    # This seems to always be at the first row, so we can remove it \n    remove_index = 0\n\n    # Create Temporary Empty Folder \n    for tomoInd in range(len(evalTomos)):\n        if (tomoInd + 1) % nProcess == rank: \n\n            # Query Run and Extract Segmentation Mask    \n            tomoID = evalTomos[tomoInd]\n            copickRun = copickRoot.get_run(tomoID)       \n            print(f'Processing Run: {tomoID} ({tomoInd})/{len(evalTomos)}')\n            labelmap = get_copick_segmentation(copickRun, segmentation_name, user_id, segmentation_session_id)[:]\n\n            # Iterate Through All Protein Classes\n            for label in range(2, n_class):\n                \n                    protein_name = label_to_name_dict.get(label)\n                    label_objs, _ = ndimage.label(labelmap == label)\n\n                    # Filter Candidates based on Object Size\n                    # Get the sizes of all objects\n                    object_sizes = np.bincount(label_objs.flat)\n\n                    # Filter the objects based on size\n                    min_object_size = 4/3 * np.pi * ((proteins[protein_name][0]/voxel_size)**2) * min_protein_size\n                    valid_objects = np.where(object_sizes > min_object_size)[0]                          \n\n                    # Estimate Coordiantes from CoM for LabelMaps\n                    deepFinderCoords = []\n                    for object_num in tqdm(valid_objects):\n                        com = ndimage.center_of_mass(label_objs == object_num)\n                        swapped_com = (com[2], com[1], com[0])\n                        deepFinderCoords.append(swapped_com)\n                    deepFinderCoords = np.array(deepFinderCoords)   \n\n                    # For some reason, consistently extracting center coordinate\n                    # Remove the row with the closest index\n                    deepFinderCoords = np.delete(deepFinderCoords, remove_index, axis=0)                    \n\n                    # Estimate Distance Threshold Based on 1/2 of Particle Diameter\n                    threshold = np.ceil(  proteins[protein_name][0] / (voxel_size * 3) )\n\n                    try: \n                        # Remove Double Counted Coordinates\n                        deepFinderCoords = eval.remove_repeated_picks(deepFinderCoords, threshold)\n\n                        # Append Euler Angles to Coordinates [ Expand Dimensions from Nx3 -> Nx6 ]\n                        deepFinderCoords = np.concatenate((deepFinderCoords, np.zeros(deepFinderCoords.shape)),axis=1)\n\n                        # Write the Starfile for Visualization\n                        if starfile_write_path is not None:\n                            tomoIDstarfilePath = os.path.join(starfile_write_path,tomoID)\n                            os.makedirs(tomoIDstarfilePath, exist_ok=True)\n                            write_relion_output(protein_name, None, deepFinderCoords, tomoIDstarfilePath , pixelSize=1) \n\n                        # Convert from Voxel to Physical Units\n                        deepFinderCoords *= voxel_size\n\n                    except Exception as e:\n                        print(f\"Error processing label {label} in tomo {tomoID}: {e}\")\n                        deepFinderCoords = np.array([]).reshape(0,6)\n\n                    # Save Picks in Copick Format / Directory \n                    write_copick_output(protein_name, tomoID, deepFinderCoords, path_output, pickMethod=user_id, sessionID = picks_session_id)\n\n    print('Extraction of Particle Coordinates Complete!')\n\ndef store_localization_parameters(predict_config, n_class, user_id, \n                                  picks_session_id,  segmentation_session_id,\n                                  segmentation_name, voxel_size, tomo_ids, path_output, \n                                  starfile_write_path, min_protein_size ):\n\n    parameters = {\n        \"input\": {\n            \"predict_config\": predict_config,\n            \"voxel_size\": voxel_size,\n            \"user_id\": user_id,\n            \"segmentation_name\": segmentation_name,\n            \"segmentation_session_id\": segmentation_session_id\n        },\n        \"output\": {\n            \"user_id\": \"deepfindET\",\n            \"picks_session_id\": picks_session_id,\n            \"min_protein_size\": min_protein_size,\n            \"path_output\": path_output,\n            \"starfile_write_path\": starfile_write_path,\n            \"tomo_ids\": tomo_ids\n        }\n    }\n    print('\\nLocalization Parameters: ', json.dumps(parameters,indent=4),'\\n')\n\n    # Save to JSON file\n    output_file = f'{user_id}_{picks_session_id}_localize_params.json'\n    with open(output_file, 'w') as json_file:\n        json.dump(parameters, json_file, indent=4)\n\n\n# if __name__ == \"__main__\": ###############\nif __name__ == \"__main__\" and not hasattr(sys, 'ps1'):\n    cli()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:03:47.381793Z","iopub.execute_input":"2025-01-30T21:03:47.382100Z","iopub.status.idle":"2025-01-30T21:03:47.400651Z","shell.execute_reply.started":"2025-01-30T21:03:47.382076Z","shell.execute_reply":"2025-01-30T21:03:47.399727Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inference\n\n# This script is adapted from a public GitHub repository.\n# Original source: https://github.com/deep-finder/cryoet-deepfinder/tree/master\n# Author: Inria,  Emmanuel Moebel, Charles Kervrann\n# License: GPL v3.0\n\n# from deepfindET.models import model_loader\n# from deepfindET.utils import core\nimport tensorflow as tf\nimport numpy as np\nimport time\n\n# Enable mixed precision\nfrom tensorflow.keras import mixed_precision\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)\n\nclass Segment(DeepFindET):\n    def __init__(self, Ncl, model_name, path_weights, patch_size=192, \n                 model_filters = [48, 64, 128], model_dropout = 0, gpuID = None):\n        DeepFindET.__init__(self)\n\n        self.Ncl = Ncl\n\n        # Segmentation, parameters for dividing data in patches:\n        self.P = patch_size  # patch length (in pixels) /!\\ has to a multiple of 4 (because of 2 pooling layers), so that dim_in=dim_out\n        self.pcrop = 25  # how many pixels to crop from border (net model dependent)\n        self.poverlap = 55  # patch overlap (in pixels) (2*pcrop + 5)\n\n        self.path_weights = path_weights\n        self.check_attributes()\n\n        # Initialize Empty network:\n        self.net = load_model(patch_size, Ncl, model_name, path_weights, \n                                           model_filters, model_dropout)[0]\n\n        # Set GPU configuration\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n        if gpus and gpuID is not None:\n            try:\n                # Restrict TensorFlow to only use the first GPU\n                tf.config.experimental.set_visible_devices(gpus[gpuID], 'GPU')\n                tf.config.experimental.set_memory_growth(gpus[gpuID], True)\n            except RuntimeError as e:\n                # Visible devices must be set at program startup\n                print(e)\n\n    # Build network:\n    def load_model(self, model_name, path_weights):\n        self.path_weights = path_weights\n        self.check_attributes()\n        self.net = model_loader.load_model(self.P, self.Ncl, model_name, path_weights)        \n\n    def check_attributes(self):\n        self.is_positive_int(self.Ncl, 'Ncl')\n        self.is_h5_path(self.path_weights, 'path_weights')\n        self.is_multiple_4_int(self.P, 'patch_size')\n\n    def launch(self, dataArray):\n        \"\"\"This function enables to segment a tomogram. As tomograms are too large to be processed in one take, the\n        tomogram is decomposed in smaller overlapping 3D patches.\n\n        Args:\n            dataArray (3D numpy array): the volume to be segmented\n            weights_path (str): path to the .h5 file containing the network weights obtained by the training procedure\n\n        Returns:\n            numpy array: contains predicted score maps. Array with index order [class,z,y,x]\n        \"\"\"\n\n        # if self.net is None:\n        #     self.load_model(self.dim_in, self.Ncl, 'unet', None)\n\n        self.check_arguments(dataArray, self.P)\n\n        dataArray = (dataArray[:] - np.mean(dataArray[:])) / np.std(dataArray[:])  # normalize\n        dataArray = np.pad(dataArray, self.pcrop, mode='constant', constant_values=0)  # zeropad\n        dim = dataArray.shape\n\n        l = np.int_(self.P / 2)\n        lcrop = np.int_(l - self.pcrop)\n        step = np.int_(2 * l + 1 - self.poverlap)\n\n        # Get patch centers:\n        pcenterX = list(range(l, dim[0] - l, step))  # list() necessary for py3\n        pcenterY = list(range(l, dim[1] - l, step))\n        pcenterZ = list(range(l, dim[2] - l, step))\n\n        # If there are still few pixels at the end:\n        if pcenterX[-1] < dim[0] - l:\n            pcenterX = pcenterX + [dim[0] - l, ]\n        if pcenterY[-1] < dim[1] - l:\n            pcenterY = pcenterY + [dim[1] - l, ]\n        if pcenterZ[-1] < dim[2] - l:\n            pcenterZ = pcenterZ + [dim[2] - l, ]\n\n        Npatch = len(pcenterX) * len(pcenterY) * len(pcenterZ)\n        self.display('Data array is divided in ' + str(Npatch) + ' patches ...')\n\n        # ---------------------------------------------------------------\n        # Process data in patches:\n        start = time.time()\n\n        predArray = np.zeros(dim + (self.Ncl,), dtype=np.float16)\n        normArray = np.zeros(dim, dtype=np.int8)\n        patchCount = 1\n        for x in pcenterX:\n            for y in pcenterY:\n                for z in pcenterZ:\n                    self.display('Segmenting patch ' + str(patchCount) + ' / ' + str(Npatch) + ' ...')\n                    patch = dataArray[x - l:x + l, y - l:y + l, z - l:z + l]\n                    patch = np.reshape(patch, (1, self.P, self.P, self.P, 1))  # reshape for keras [batch,x,y,z,channel]\n                    pred = self.net.predict(patch, batch_size=1)\n\n                    predArray[x - lcrop:x + lcrop, y - lcrop:y + lcrop, z - lcrop:z + lcrop, :] = predArray[\n                                                                                                  x - lcrop:x + lcrop,\n                                                                                                  y - lcrop:y + lcrop,\n                                                                                                  z - lcrop:z + lcrop,\n                                                                                                  :] + np.float16(pred[0,\n                                                                                                       l - lcrop:l + lcrop,\n                                                                                                       l - lcrop:l + lcrop,\n                                                                                                       l - lcrop:l + lcrop,\n                                                                                                       :])\n                    normArray[x - lcrop:x + lcrop, y - lcrop:y + lcrop, z - lcrop:z + lcrop] = normArray[\n                                                                                               x - lcrop:x + lcrop,\n                                                                                               y - lcrop:y + lcrop,\n                                                                                               z - lcrop:z + lcrop] + np.ones(\n                        (self.P - 2 * self.pcrop, self.P - 2 * self.pcrop, self.P - 2 * self.pcrop), dtype=np.int8)\n\n                    patchCount += 1\n\n        # Normalize overlaping regions:\n        for C in range(0, self.Ncl):\n            predArray[:, :, :, C] = predArray[:, :, :, C] / normArray\n\n        end = time.time()\n        self.display(\"Model took %0.2f seconds to predict\" % (end - start))\n\n        predArray = predArray[self.pcrop:-self.pcrop, self.pcrop:-self.pcrop, self.pcrop:-self.pcrop, :]  # unpad\n        return predArray  # predArray is the array containing the scoremaps\n\n\n    # Similar to function 'segment', only here the tomogram is not decomposed in smaller patches, but processed in one take. However, the tomogram array should be cubic, and the cube length should be a multiple of 4. This function has been developped for tests on synthetic data. I recommend using 'segment' rather than 'segment_single_block'.\n    # INPUTS:\n    #   dataArray: the volume to be segmented (3D numpy array)\n    #   weights_path: path to the .h5 file containing the network weights obtained by the training procedure (string)\n    # OUTPUT:\n    #   predArray: a numpy array containing the predicted score maps.\n    def launch_single_block(self, dataArray):\n        self.check_arguments(dataArray, self.P)\n\n        dataArray = (dataArray[:] - np.mean(dataArray[:])) / np.std(dataArray[:])  # normalize\n        dataArray = np.pad(dataArray, self.pcrop, mode='constant')  # zeropad\n        dim = dataArray.shape\n        dataArray = np.reshape(dataArray, (1, dim[0], dim[1], dim[2], 1))  # reshape for keras [batch,x,y,z,channel]\n\n        pred = self.net.predict(dataArray, batch_size=1)\n        predArray = pred[0, :, :, :, :]\n        #predArray = predArray[self.pcrop:-self.pcrop, self.pcrop:-self.pcrop, self.pcrop:-self.pcrop, :]  # unpad\n\n        return predArray\n\n    def check_arguments(self, dataArray, patch_size):\n        self.is_3D_nparray(dataArray, 'tomogram')\n        self.check_array_minsize([dataArray, patch_size], ['tomogram', 'patch'])\n\n    def to_labelmap(self, scoremaps):\n        \"\"\"Converts scoremaps into a labelmap.\n\n        Args:\n            scoremaps (4D numpy array): array with index order [class,z,y,x]\n\n        Returns:\n            3D numpy array: array with index order [z,y,x]\n        \"\"\"\n        labelmap = np.int8( np.argmax(scoremaps,3) )\n        return labelmap\n\n# if __name__ == \"__main__\": ##################\nif __name__ == \"__main__\" and not hasattr(sys, 'ps1'):\n    cli()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:03:54.012701Z","iopub.execute_input":"2025-01-30T21:03:54.013002Z","iopub.status.idle":"2025-01-30T21:03:54.029892Z","shell.execute_reply.started":"2025-01-30T21:03:54.012980Z","shell.execute_reply":"2025-01-30T21:03:54.028973Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make a copick project\n\nconfig_blob = \"\"\"{\n    \"name\": \"czii_cryoet_mlchallenge_2024\",\n    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n    \"version\": \"1.0.0\",\n\n    \"pickable_objects\": [\n        {\n            \"name\": \"apo-ferritin\",\n            \"is_particle\": true,\n            \"pdb_id\": \"4V1W\",\n            \"label\": 1,\n            \"color\": [  0, 117, 220, 128],\n            \"radius\": 60,\n            \"map_threshold\": 0.0418\n        },\n        {\n            \"name\": \"beta-amylase\",\n            \"is_particle\": true,\n            \"pdb_id\": \"1FA2\",\n            \"label\": 2,\n            \"color\": [153,  63,   0, 128],\n            \"radius\": 65,\n            \"map_threshold\": 0.035\n        },\n        {\n            \"name\": \"beta-galactosidase\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6X1Q\",\n            \"label\": 3,\n            \"color\": [ 76,   0,  92, 128],\n            \"radius\": 90,\n            \"map_threshold\": 0.0578\n        },\n        {\n            \"name\": \"ribosome\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6EK0\",\n            \"label\": 4,\n            \"color\": [  0,  92,  49, 128],\n            \"radius\": 150,\n            \"map_threshold\": 0.0374\n        },\n        {\n            \"name\": \"thyroglobulin\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6SCJ\",\n            \"label\": 5,\n            \"color\": [ 43, 206,  72, 128],\n            \"radius\": 130,\n            \"map_threshold\": 0.0278\n        },\n        {\n            \"name\": \"virus-like-particle\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6N4V\",            \n            \"label\": 6,\n            \"color\": [255, 204, 153, 128],\n            \"radius\": 135,\n            \"map_threshold\": 0.201\n        }\n    ],\n\n    \"overlay_root\": \"/kaggle/working/test/overlay\",\n\n    \"overlay_fs_args\": {\n        \"auto_mkdir\": true\n    },\n\n    \"static_root\": \"/kaggle/input/czii-cryo-et-object-identification/test/static\"\n}\"\"\"\n\ncopick_config_path = \"/kaggle/working/copick.config\"\n\nwith open(copick_config_path, \"w\") as f:\n    f.write(config_blob)\n    \n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-01-30T21:03:58.145090Z","iopub.execute_input":"2025-01-30T21:03:58.145496Z","iopub.status.idle":"2025-01-30T21:03:58.149937Z","shell.execute_reply.started":"2025-01-30T21:03:58.145469Z","shell.execute_reply":"2025-01-30T21:03:58.149067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from deepfindET.entry_points import step3\n# from deepfindET.utils import copick_tools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport copick\n\n%matplotlib inline\n\n################## Input Parameters #################\n\n# Config File\nconfig = '/kaggle/working/copick.config'\n\n# Model Parameters\nn_class = 8                 # Number of classes to predict.\npatch_size = 160            # Size of the input patch fed into the model for inference.\nmodel_name = 'res_unet'     # The model architecture used for training.\n# filters = [48, 64, 128]      # Number of filters for U-Net (same parameter as used for training).\nfilters = [12, 16, 32]      # Number of filters for U-Net (same parameter as used for training).\ndropout = 0                 # Dropout rate applied during inference.\n\n# Path to the pre-trained model weights for the chosen architecture.\nmodel_weights = '/kaggle/input/a-very-bad-model/other/default/1/a_very_bad_model.h5'\n\n# Query for Tomogram\nvoxel_size = 10             # Resolution of the tomogram in voxel size.\ntomogram_algorithm = 'denoised'  # Reconstruction algorithm used for generating the tomogram\n\n# Output Segmentation Write Name\nsegmentation_name = 'predict'\nsession_id = '0'\nuser_id = 'deepfindET'\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-01-30T21:04:05.871364Z","iopub.execute_input":"2025-01-30T21:04:05.871722Z","iopub.status.idle":"2025-01-30T21:04:05.878204Z","shell.execute_reply.started":"2025-01-30T21:04:05.871692Z","shell.execute_reply":"2025-01-30T21:04:05.877256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the train DeepFindET 3D U-Net model on the copick directory.\ninference_tomogram_segmentation(\n    config,                                 # Copick Configuration File\n    n_class,                                # Number of classes to predict.\n    model_name,                             # The model architecture used for training.\n    model_weights,                          # Path to the pre-trained model weights for the chosen architecture.\n    patch_size,                             # Size of the input patch fed into the model for inference.\n    user_id,                                # Identifier of the user or project running the segmentation.\n    session_id,                             # Session identifier for tracking purposes.\n    segmentation_name=segmentation_name,     # Identifier for the output segmentation file.\n    voxel_size = voxel_size,                # Voxel Size of Tomogram to Run Inference On       \n    model_filters = filters,                # Number of filters for U-Net\n    model_dropout = dropout,                # Dropout rate\n    tomogram_algorithm= tomogram_algorithm, # Reconstruction algorithm used for generating the tomogram\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:04:14.413828Z","iopub.execute_input":"2025-01-30T21:04:14.414250Z","iopub.status.idle":"2025-01-30T21:07:09.460648Z","shell.execute_reply.started":"2025-01-30T21:04:14.414210Z","shell.execute_reply":"2025-01-30T21:07:09.459734Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from deepfindET.entry_points import step4\n\ncopick_root = copick.from_file(config)\n\n# Session ID for the Output Picks\npicks_session_id = '0'\n\nsegmentation_session_id = '0'\n\nsegmentation_name = 'predict' \n\nmin_protein_size = 0.4  \n\npath_output = f\"{copick_root.root_overlay}/ExperimentRuns\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:08:00.705959Z","iopub.execute_input":"2025-01-30T21:08:00.706280Z","iopub.status.idle":"2025-01-30T21:08:00.711265Z","shell.execute_reply.started":"2025-01-30T21:08:00.706258Z","shell.execute_reply":"2025-01-30T21:08:00.710466Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[obj.radius for obj in copick_root.pickable_objects]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:08:03.440042Z","iopub.execute_input":"2025-01-30T21:08:03.440412Z","iopub.status.idle":"2025-01-30T21:08:03.446366Z","shell.execute_reply.started":"2025-01-30T21:08:03.440361Z","shell.execute_reply":"2025-01-30T21:08:03.445683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install pycm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:08:07.544424Z","iopub.execute_input":"2025-01-30T21:08:07.544714Z","iopub.status.idle":"2025-01-30T21:08:11.899797Z","shell.execute_reply.started":"2025-01-30T21:08:07.544694Z","shell.execute_reply":"2025-01-30T21:08:11.898782Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluate\nfrom scipy.cluster.hierarchy import fcluster, linkage\nfrom sklearn.metrics import pairwise_distances\n# from deepfindET.utils import objl as ol\nfrom scipy.spatial import distance\nfrom pycm import ConfusionMatrix\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport copy\n\ndef remove_repeated_picks_v2(coordinates, distanceThreshold, pixelSize = 1):\n\n    # Calculate the distance matrix for the 3D coordinates\n    dist_matrix = distance.cdist(coordinates[:, :3]/pixelSize, coordinates[:, :3]/pixelSize)\n\n    # Create a linkage matrix using single linkage method\n    Z = linkage(dist_matrix, method='complete')\n\n    # Form flat clusters with a distance threshold to determine groups\n    clusters = fcluster(Z, t=distanceThreshold, criterion='distance')\n\n    # Initialize an array to store the average of each group\n    unique_coordinates = np.zeros((max(clusters), coordinates.shape[1]))\n\n    # Calculate the mean for each cluster\n    for i in range(1, max(clusters) + 1):\n        unique_coordinates[i-1] = np.mean(coordinates[clusters == i], axis=0)\n\n    return unique_coordinates    \n\ndef remove_repeated_picks(coordinates, distanceThreshold, pixelSize = 1):\n\n    # Compute pairwise distances\n    dist_matrix = pairwise_distances(coordinates)\n    \n    # Use hierarchical clustering to group close points\n    distanceThreshold = distanceThreshold**2\n    linkage_matrix = linkage(dist_matrix, method='single', metric='euclidean')\n    labels = fcluster(linkage_matrix, t=distanceThreshold, criterion='distance')\n    \n    # Calculate the average coordinates for each group\n    unique_labels = np.unique(labels)\n    filteredCoordinates = np.array([coordinates[labels == label].mean(axis=0) for label in unique_labels])\n\n    return filteredCoordinates \n\ndef compute_metrics(gt_points, pred_points, threshold):\n    gt_points = np.array(gt_points)\n    pred_points = np.array(pred_points)\n    \n    # Calculate distances\n    dist_matrix = distance.cdist(pred_points, gt_points, 'euclidean')\n\n    # Determine matches within the threshold\n    tp = np.sum(np.min(dist_matrix, axis=1) < threshold)\n    fp = np.sum(np.min(dist_matrix, axis=1) >= threshold)\n    fn = np.sum(np.min(dist_matrix, axis=0) >= threshold)\n    \n    # Precision, Recall, F1 Score\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    accuracy = tp / (tp + fp + fn)  # Note: TN not considered here\n    \n    return {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'accuracy': accuracy, \n        'true_positives': int(tp),\n        'false_positives': int(fp),\n        'false_negatives': int(fn)\n    }\n\nclass Evaluator:\n\n    def __init__(self, dset_true, dset_pred, dist_thr):\n        if dset_true.keys() != dset_pred.keys():\n            raise Exception('Data sets do not have the same keys')\n        self.dset_true = dset_true\n        self.dset_pred = dset_pred\n        self.dist_thr = dist_thr\n\n        self.tomoid_keys = dset_true.keys()\n\n        # detect_eval is a dict who stores, per tomo, all info related to the evaluation of the detections\n        self.detect_eval = {}\n\n        # Initialize detect_eval:\n        for tomoid in dset_true.keys():\n            self.detect_eval[tomoid] = {\n                'objl_true': None,\n                'objl_pred': None,\n                'dmat': None,\n                'cmat': None,\n                'objl_tp': None,\n                'objl_fp': None,\n                'objl_fn': None,\n                'pre': None,\n                'rec': None,\n                'f1s': None,\n            }\n        self.detect_eval['global'] = {\n            'cmat': None,\n            'pre': None,\n            'rec': None,\n            'f1s': None,\n            'n_tp': None,\n            'n_fp': None,\n            'n_fn': None,\n        }\n\n        # detect_eval_list stores all the detect_eval w.r.t score_thr_list\n        self.detect_eval_list = []\n\n    def get_evaluation_wrt_detection_score(self, score_thr_list):\n        for score_thr in score_thr_list:\n            self.get_evaluation(score_thr)\n            self.detect_eval_list.append(copy.deepcopy(self.detect_eval))\n\n        return self.detect_eval_list\n\n    def get_evaluation(self, score_thr=None):\n        for tomoid in self.tomoid_keys:\n            objl_true = self.dset_true[tomoid]['object_list']\n            objl_pred = self.dset_pred[tomoid]['object_list']\n            if score_thr is not None:\n                objl_pred = ol.above_thr(objl_pred, score_thr)\n            self.detect_eval[tomoid]['objl_true'] = objl_true\n            self.detect_eval[tomoid]['objl_pred'] = objl_pred\n\n            self.get_distance_matrix(tomoid)\n\n        self.get_confusion_matrix()\n        self.get_metrics()\n\n        return self.detect_eval\n\n    def get_distance_matrix(self, tomoid):\n        # Prepare data points for pairwise_distances:\n        objl_true = self.detect_eval[tomoid]['objl_true']\n        objl_pred = self.detect_eval[tomoid]['objl_pred']\n\n        coords_true = np.zeros((len(objl_true), 3))\n        coords_pred = np.zeros((len(objl_pred), 3))\n        for idx, obj in enumerate(objl_true):\n            coords_true[idx, 0] = obj['y']\n            coords_true[idx, 1] = obj['x']\n            coords_true[idx, 2] = obj['z']\n        for idx, obj in enumerate(objl_pred):\n            coords_pred[idx, 0] = obj['y']\n            coords_pred[idx, 1] = obj['x']\n            coords_pred[idx, 2] = obj['z']\n\n        # Compute pairwise distances:\n        if len(objl_pred) is not 0:  # only compute dmat if something has been detected\n            dmat = pairwise_distances(coords_true, coords_pred, metric='euclidean')\n        else:  # if no detections then dmat is not defined (and later on all scores are 0)\n            dmat = None\n\n        self.detect_eval[tomoid]['dmat'] = dmat\n\n    def get_confusion_matrix(self):\n        y_true_global = []\n        y_pred_global = []\n        for tomoid in self.tomoid_keys:\n            dmat = self.detect_eval[tomoid]['dmat']\n            objl_true = self.detect_eval[tomoid]['objl_true'].copy()\n            objl_pred = self.detect_eval[tomoid]['objl_pred'].copy()\n            if dmat is not None:  # i.e., if something has been detected\n                # The aim of this part is to construct objl_pred_corresp and objl_true_corresp\n                # from objl_pred and objl_true, such that objl_pred_corresp[i] matches objl_true_corresp[i]\n                # (matches = same position with a tolerated position error of self.dist_thr)\n\n                # Initialize objl_pred_corresp\n                objl_pred_corresp = [None for _ in range(len(objl_true))]\n                objl_true_corresp = objl_true.copy()\n\n                # Correspondence matrix where '1' means that an entry from pred is situated at a distance <=dist_thr\n                # to an entry of true:\n                corresp_mat = dmat <= self.dist_thr\n\n                pred_match_idx_list = []  # this list stores all the entries in objl_pred that have a match in objl_true\n                for idx, obj in enumerate(objl_true_corresp):\n                    indices = np.nonzero(corresp_mat[idx, :])[0]\n                    if len(indices) == 1:  # tp only if 1 corresponence (necessary in case 1 true matches several pred)\n                        objl_pred_corresp[idx] = objl_pred[indices[0]]\n                        pred_match_idx_list.append(indices[0])\n                    elif len(indices) < 1 or len(indices) > 1:  # if no correspondence or multiple correspondence, then fp\n                        obj_fp = copy.deepcopy(obj)  # necessary else the labels in objl_true get modified\n                        obj_fp['label'] = 0  # fp, therefore label is 0\n                        objl_pred_corresp[idx] = obj_fp\n                    else:\n                        print('Exception!! This case should never happen.')\n\n                # First, get objl_tp:\n                pred_match_idx_list = np.unique(pred_match_idx_list)  # necessary in case 1 pred matches several true. Therefore 1 pred is counted only once as tp\n                pred_match_idx_list = np.flip(np.sort(pred_match_idx_list))  # sort idx in descending order, else it is a mess when deleting elements\n\n                # Get all predictions that have a match in objl_true (i.e., objl_tp):\n                objl_tp = []\n                for idx in pred_match_idx_list:\n                    objl_tp.append(objl_pred[idx])\n\n                # Get all predictions that do not have a match in objl_true:\n                objl_pred_no_corresp = copy.deepcopy(objl_pred) # initialize\n                for idx in pred_match_idx_list:\n                    del objl_pred_no_corresp[idx]\n\n                # All obj in objl_pred_no_corresp are false positives. So we add them in objl_true_no_corresp with label 0\n                objl_true_no_corresp = copy.deepcopy(objl_pred_no_corresp)\n                for idx, obj in enumerate(objl_true_no_corresp):\n                    obj['label'] = 0\n\n                # Finally:\n                objl_true_corresp = objl_true_corresp + objl_true_no_corresp\n                objl_pred_corresp = objl_pred_corresp + objl_pred_no_corresp\n\n                # Now it is easy to get objl_fp and objl_fn:\n                objl_fp = ol.get_class(objl_true_corresp, label=0)\n                objl_fn = ol.get_class(objl_pred_corresp, label=0)\n\n                # Prepare the inputs of ConfusionMatrix:\n                y_true = [obj['label'] for obj in objl_true_corresp]\n                y_pred = [obj['label'] for obj in objl_pred_corresp]\n\n            else:  # if nothing has been detected\n                objl_true_corresp = objl_true.copy()\n                y_true = [obj['label'] for obj in objl_true_corresp]\n                y_pred = [0 for _ in range(len(y_true))]\n\n                objl_tp = []\n                objl_fp = []\n                objl_fn = objl_true.copy()\n\n            # Get ConfusionMatrix:\n            cmat = ConfusionMatrix(actual_vector=y_true, predict_vector=y_pred)\n\n            # Store:\n            self.detect_eval[tomoid]['cmat'] = cmat\n            self.detect_eval[tomoid]['objl_tp'] = objl_tp\n            self.detect_eval[tomoid]['objl_fp'] = objl_fp\n            self.detect_eval[tomoid]['objl_fn'] = objl_fn\n\n            y_true_global += y_true\n            y_pred_global += y_pred\n\n        cmat_global = ConfusionMatrix(actual_vector=y_true_global, predict_vector=y_pred_global)\n        self.detect_eval['global']['cmat'] = cmat_global\n\n    def get_metrics(self):\n        n_true_global = 0\n        n_pred_global = 0\n        for tomoid in self.tomoid_keys:\n            self.detect_eval[tomoid]['pre'] = self.detect_eval[tomoid]['cmat'].PPV\n            self.detect_eval[tomoid]['rec'] = self.detect_eval[tomoid]['cmat'].TPR\n            self.detect_eval[tomoid]['f1s'] = self.detect_eval[tomoid]['cmat'].F1\n            self.detect_eval[tomoid]['n_tp'] = self.detect_eval[tomoid]['cmat'].TP\n            self.detect_eval[tomoid]['n_fp'] = self.detect_eval[tomoid]['cmat'].FP\n            self.detect_eval[tomoid]['n_fn'] = self.detect_eval[tomoid]['cmat'].FN\n\n            n_true = len(self.detect_eval[tomoid]['objl_true'])\n            n_pred = len(self.detect_eval[tomoid]['objl_pred'])\n            self.detect_eval[tomoid]['n_true'] = n_true\n            self.detect_eval[tomoid]['n_pred'] = n_pred\n\n            n_true_global += n_true\n            n_pred_global += n_pred\n\n\n        self.detect_eval['global']['pre'] = self.detect_eval['global']['cmat'].PPV\n        self.detect_eval['global']['rec'] = self.detect_eval['global']['cmat'].TPR\n        self.detect_eval['global']['f1s'] = self.detect_eval['global']['cmat'].F1\n        self.detect_eval['global']['n_tp'] = self.detect_eval['global']['cmat'].TP\n        self.detect_eval['global']['n_fp'] = self.detect_eval['global']['cmat'].FP\n        self.detect_eval['global']['n_fn'] = self.detect_eval['global']['cmat'].FN\n\n        self.detect_eval['global']['n_true'] = n_true_global\n        self.detect_eval['global']['n_pred'] = n_pred_global\n\n    def to_labelmap(self, scoremaps):\n        \"\"\"Converts scoremaps into a labelmap.\n\n        Args:\n            scoremaps (4D numpy array): array with index order [class,z,y,x]\n\n        Returns:\n            3D numpy array: array with index order [z,y,x]\n        \"\"\"\n        labelmap = np.int8( np.argmax(scoremaps,3) )\n        return labelmap\n\n\ndef plot_eval(detect_eval, class_label, score_thr_list):\n    pre_list = []\n    rec_list = []\n    f1s_list = []\n    for deval in detect_eval:\n        pre = deval['global']['pre'][class_label]\n        rec = deval['global']['rec'][class_label]\n        f1s = deval['global']['f1s'][class_label]\n\n        pre_list.append(pre)\n        rec_list.append(rec)\n        f1s_list.append(f1s)\n\n    # Get max(F1-score) with corresponding precision and recall:\n    f1s_max = np.max(f1s_list)\n    idx_max = np.argmax(f1s_list)\n    pre_best = pre_list[idx_max]\n    rec_best = rec_list[idx_max]\n\n    f1s_max = np.round(f1s_max, 2)\n    pre_best = np.round(pre_best, 2)\n    rec_best = np.round(rec_best, 2)\n\n    # Drop the plot:\n    fontsize = 10\n    fig, ax = plt.subplots(1, 1)\n    ax.plot(score_thr_list, pre_list)\n    ax.plot(score_thr_list, rec_list)\n    ax.plot(score_thr_list, f1s_list)\n    ax.set_title(f'max(f1-score)={f1s_max}, [pre,rec]=[{pre_best},{rec_best}]', fontsize=fontsize)\n    ax.set_xlim([np.min(score_thr_list), np.max(score_thr_list)])\n    ax.set_ylim([0, 1])\n    ax.set_xlabel('Detection score threshold')\n    ax.set_ylabel('Metrics')\n    ax.grid(True)\n    ax.legend(['Precision', 'Recall', 'F1-score'])\n\n    return fig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:08:22.652471Z","iopub.execute_input":"2025-01-30T21:08:22.652808Z","iopub.status.idle":"2025-01-30T21:08:23.190505Z","shell.execute_reply.started":"2025-01-30T21:08:22.652782Z","shell.execute_reply":"2025-01-30T21:08:23.189477Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Derived from https://github.com/copick/DeepFindET/blob/main/deepfindET/entry_points/step4.py\n\n# import deepfindET.utils.copick_tools as tools\n# import deepfindET.utils.evaluate as evaluate\nimport scipy.ndimage as ndimage\nfrom tqdm import tqdm\n\n# Currently Filtering Process always finds coordinate at (cx,cy,cz) - center coordinate\n# This seems to always be at the first row, so we can remove it \nremove_index = 0\n\n# Extract Protein Coordinates from the Segmentation Masks\ndef extract_coords(pickable_object, copick_run):\n    labelmap = get_copick_segmentation(copick_run, segmentation_name, user_id, segmentation_session_id)[:]\n    label = pickable_object.label\n    protein_name = pickable_object.name\n    label_objs, _ = ndimage.label(labelmap == label)\n\n    # Filter Candidates based on Object Size\n    # Get the sizes of all objects\n    object_sizes = np.bincount(label_objs.flat)\n\n    # Filter the objects based on size\n    min_object_size = 4/3 * np.pi * ((pickable_object.radius/voxel_size)**2) * min_protein_size\n    valid_objects = np.where(object_sizes > min_object_size)[0]                          \n\n    # Estimate Coordiantes from CoM for LabelMaps\n    deepFinderCoords = []\n    for object_num in tqdm(valid_objects):\n        com = ndimage.center_of_mass(label_objs == object_num)\n        swapped_com = (com[2], com[1], com[0])\n        deepFinderCoords.append(swapped_com)\n    deepFinderCoords = np.array(deepFinderCoords)   \n\n    # For some reason, consistently extracting center coordinate\n    # Remove the row with the closest index\n    deepFinderCoords = np.delete(deepFinderCoords, remove_index, axis=0)                    \n\n    # Estimate Distance Threshold Based on 1/2 of Particle Diameter\n    threshold = np.ceil(  pickable_object.radius / (voxel_size * 3) )\n\n    try: \n        # Remove Double Counted Coordinates\n        deepFinderCoords = remove_repeated_picks(deepFinderCoords, threshold)\n\n        # Append Euler Angles to Coordinates [ Expand Dimensions from Nx3 -> Nx6 ]\n        deepFinderCoords = np.concatenate((deepFinderCoords, np.zeros(deepFinderCoords.shape)),axis=1)\n\n        # Convert from Voxel to Physical Units\n        deepFinderCoords *= voxel_size\n\n    except Exception as e:\n        print(f\"Error processing label {label} in tomo {copick_run}: {e}\")\n        deepFinderCoords = np.array([]).reshape(0,6)\n\n    # Save Picks in Copick Format / Directory \n    write_copick_output(protein_name, copick_run.meta.name, deepFinderCoords, path_output, pickMethod=user_id, sessionID = picks_session_id)\n    \n    \nfor run in copick_root.runs:\n    print(f\"Run {run}\")\n    for pickable_object in copick_root.pickable_objects:\n        print(pickable_object.name)\n        if pickable_object.is_particle:\n            extract_coords(pickable_object, run)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:09:06.041260Z","iopub.execute_input":"2025-01-30T21:09:06.043433Z","iopub.status.idle":"2025-01-30T21:39:30.753568Z","shell.execute_reply.started":"2025-01-30T21:09:06.043391Z","shell.execute_reply":"2025-01-30T21:39:30.752607Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nimport os\n\nos.listdir(\"/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns\")\n\nresults = []\npick_id = 0\n\n# id,experiment,particle_type,x,y,z\nfor run in copick_root.runs:\n    run_id = run.meta.name\n    for particle_type in copick_root.pickable_objects:\n        picks = run.get_picks(particle_type.name, user_id=\"deepfindET\")\n        if picks:\n            picks = picks[0]\n            points = picks.points\n            for point in points:                \n                row = [pick_id, run_id, particle_type.name, point.location.x, point.location.y, point.location.z]\n                results.append(row)\n                pick_id += 1\n\nprint(f\"Found {len(results)} picks\")\n\n# Define CSV output file path\noutput_csv_path = \"/kaggle/working/submission.csv\"\n\n# Write results to CSV\nwith open(output_csv_path, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    # Write header\n    writer.writerow([\"id\", \"experiment\", \"particle_type\", \"x\", \"y\", \"z\"])\n    # Write data rows\n    writer.writerows(results)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T21:40:01.218397Z","iopub.execute_input":"2025-01-30T21:40:01.218724Z","iopub.status.idle":"2025-01-30T21:40:01.358712Z","shell.execute_reply.started":"2025-01-30T21:40:01.218695Z","shell.execute_reply":"2025-01-30T21:40:01.357782Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}